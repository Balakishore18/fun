{
  "name": "MyJobName",
  "new_cluster": {
    "spark_version": "7.3.x-scala2.12",
    "node_type_id": "Standard_DS3_v2",
    "num_workers": 2
  },
  "notebook_task": {
    "notebook_path": "/Workspace/path/to/your/notebook"
  }
}


import subprocess
import json

def create_and_run_databricks_job(config_file):
    # Create a job using the JSON file
    create_job_command = [
        "databricks", "jobs", "create",
        "--json-file", config_file
    ]
    
    response = subprocess.run(create_job_command, capture_output=True, text=True)
    if response.returncode != 0:
        raise Exception(f"Error creating job: {response.stderr}")
    
    # Parse job id from the response
    response_json = json.loads(response.stdout)
    job_id = response_json["job_id"]
    
    # Run the job
    run_job_command = [
        "databricks", "jobs", "run-now",
        "--job-id", str(job_id)
    ]
    
    response = subprocess.run(run_job_command, capture_output=True, text=True)
    if response.returncode != 0:
        raise Exception(f"Error running job: {response.stderr}")
    
    print(response.stdout)

# Example usage
if __name__ == "__main__":
    config_file = "config.json"
    create_and_run_databricks_job(config_file)


import json
from azure.storage.blob import BlobServiceClient
from azure.cosmos import CosmosClient, PartitionKey

# Step 1: Fetch JSON from Azure Blob Storage

def fetch_blob_data(blob_account_url, container_name, blob_name, blob_sas_token):
    blob_service_client = BlobServiceClient(account_url=blob_account_url, credential=blob_sas_token)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    blob_data = blob_client.download_blob().readall()
    return json.loads(blob_data)

# Step 2: Write the JSON data to Cosmos DB

def write_data_to_cosmos(cosmos_url, cosmos_key, database_name, container_name, data):
    cosmos_client = CosmosClient(cosmos_url, cosmos_key)
    database = cosmos_client.get_database_client(database_name)
    
    # Assuming the container is already created with a partition key as needed
    container = database.get_container_client(container_name)
    
    container.upsert_item(data)

# Usage:

blob_account_url = "https://your_blob_account_name.blob.core.windows.net/"
container_name = "your_container_name"
blob_name = "your_blob_name.json"
blob_sas_token = "your_blob_sas_token"

data = fetch_blob_data(blob_account_url, container_name, blob_name, blob_sas_token)

cosmos_url = "https://your_cosmos_account_name.documents.azure.com:443/"
cosmos_key = "your_cosmos_primary_or_secondary_key"
database_name = "your_database_name"
container_name = "your_container_name"

write_data_to_cosmos(cosmos_url, cosmos_key, database_name, container_name, data)


from azure.storage.blob import BlobServiceClient
from azure.identity import DefaultAzureCredential

def fetch_blob_data_with_managed_identity(account_url, container_name, blob_name):
    credential = DefaultAzureCredential()
    blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    blob_data = blob_client.download_blob().readall()
    return json.loads(blob_data)

# Example usage:
account_url = 'https://your_account_name.blob.core.windows.net'
container_name = 'your_container_name'
blob_name = 'your_blob_name.json'
data = fetch_blob_data_with_managed_identity(account_url, container_name, blob_name)

pip install azure-storage-blob azure-identity


from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
import json

#connection_string = "https://balaproject.blob.core.windows.net"
connection_string = "DefaultEndpointsProtocol=https;AccountName=balaproject;AccountKey=kO5r0tP4/XHfFaeiP/P2jwvIqoeQu5HhvlwF6N6xJqEwBie5NyUEZ5ozY921I+TFz9p8QuTSMMnm+ASthbLlfQ==;EndpointSuffix=core.windows.net"

container_name = "bala"

blob_service_client = BlobServiceClient.from_connection_string(connection_string)
container_client = blob_service_client.get_container_client(container_name)

for blob in container_client.list_blobs():
    if blob.name.endswith('.json'):
        blob_client = container_client.get_blob_client(blob.name)
        blob_data = blob_client.download_blob()
        json_data = json.loads(blob_data.readall())
        print(json_data)


{
  "config_id": "configForReadBlobJob",
  "name": "readBlobJob",
  "existing_cluster_id": "YOUR_EXISTING_CLUSTER_ID",
  "notebook_task": {
    "notebook_path": "/Users/bhongarastacestreet.com/readingBlobJob"
  }
}
