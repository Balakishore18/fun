{
  "name": "MyJobName",
  "new_cluster": {
    "spark_version": "7.3.x-scala2.12",
    "node_type_id": "Standard_DS3_v2",
    "num_workers": 2
  },
  "notebook_task": {
    "notebook_path": "/Workspace/path/to/your/notebook"
  }
}


import subprocess
import json

def create_and_run_databricks_job(config_file):
    # Create a job using the JSON file
    create_job_command = [
        "databricks", "jobs", "create",
        "--json-file", config_file
    ]
    
    response = subprocess.run(create_job_command, capture_output=True, text=True)
    if response.returncode != 0:
        raise Exception(f"Error creating job: {response.stderr}")
    
    # Parse job id from the response
    response_json = json.loads(response.stdout)
    job_id = response_json["job_id"]
    
    # Run the job
    run_job_command = [
        "databricks", "jobs", "run-now",
        "--job-id", str(job_id)
    ]
    
    response = subprocess.run(run_job_command, capture_output=True, text=True)
    if response.returncode != 0:
        raise Exception(f"Error running job: {response.stderr}")
    
    print(response.stdout)

# Example usage
if __name__ == "__main__":
    config_file = "config.json"
    create_and_run_databricks_job(config_file)
