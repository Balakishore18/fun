# Import necessary libraries
from azure.cosmos import CosmosClient
from pyspark.sql import SparkSession
import requests
import json
import os

# ----- Cosmos DB Connection and Configuration Retrieval -----
def get_cosmosdb_config():
    url = os.getenv('COSMOS_DB_URI')
    key = os.getenv('COSMOS_DB_KEY')
    database_name = 'your_database_name'
    container_name = 'your_container_name'

    client = CosmosClient(url, credential=key)
    database = client.get_database_client(database_name)
    container = database.get_container_client(container_name)

    query = "SELECT * FROM c WHERE c.id = 'config_id'"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    return items[0] if items else None

# ----- Delta Lake Table Connection -----
def connect_to_delta_lake(delta_table_path):
    spark = SparkSession.builder.appName("DeltaLakeConnection").getOrCreate()
    df = spark.read.format("delta").load(delta_table_path)
    return df

# ----- Change Data Feed Reading -----
def read_change_data_feed(delta_table_path):
    spark = SparkSession.builder.appName("ReadChangeData").getOrCreate()
    change_df = spark.read.format("delta").option("readChangeData", "true").table(delta_table_path)
    return change_df

# ----- Schema Comparison -----
def compare_schemas(current_schema, stored_schema):
    return str(current_schema) != str(stored_schema)

# ----- Databricks Job Triggering -----
def trigger_databricks_job(databricks_instance, job_id):
    databricks_token = os.getenv('DATABRICKS_TOKEN')
    url = f"{databricks_instance}/api/2.0/jobs/run-now"
    headers = {
        'Authorization': f'Bearer {databricks_token}',
        'Content-Type': 'application/json'
    }
    payload = {"job_id": job_id}

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to trigger job: {response.text}")
        return None

# ----- Main Application Flow -----
def main():
    # Step 1: Retrieve configuration from Cosmos DB
    config = get_cosmosdb_config()
    delta_table_path = config['deltaTableConfig']['tablePath']

    # Step 2: Connect to Delta Lake Table
    delta_df = connect_to_delta_lake(delta_table_path)

    # Step 3: Read Change Data Feed
    change_data = read_change_data_feed(delta_table_path)

    # Step 4: Schema Comparison
    # Assuming 'stored_schema' is retrieved from somewhere (e.g., Cosmos DB)
    stored_schema = config['deltaTableConfig']['schema']
    if compare_schemas(delta_df.schema, stored_schema):
        print("Schema has changed.")

    # Step 5: Trigger Databricks Job
    # Assuming job details are retrieved from the config
    databricks_instance = config['databricksJobConfig']['instance']
    job_id = config['databricksJobConfig']['jobId']
    trigger_response = trigger_databricks_job(databricks_instance, job_id)

    print(f"Databricks job trigger response: {trigger_response}")

if __name__ == "__main__":
    main()


You're right, the consolidated script I provided earlier didn't include the Azure Functions Timer Trigger code. This is because Azure Functions are typically deployed as separate entities in the cloud, and they are not usually part of a standalone Python script. However, I can guide you through creating an Azure Function with a Timer Trigger that would fit into your workflow.

Azure Function with Timer Trigger
Azure Functions with a Timer Trigger are used to execute tasks on a schedule. Hereâ€™s how you can create one:

1. Create an Azure Function App:
You can do this in the Azure portal or using the Azure CLI.
Make sure you have Azure Functions Core Tools installed if you're doing this locally.
2. Create a Function with a Timer Trigger:
Using the Azure Functions Core Tools, you can create a function with a timer trigger using the following command:

bash
Copy code
func new --name MyTimerTrigger --template "Timer trigger" --authlevel anonymous
3. Implement the Function:
This function will be triggered as per the schedule defined in its configuration.

The Python code for the function might look like this:

python
Copy code
import datetime
import logging

import azure.functions as func

def main(mytimer: func.TimerRequest) -> None:
    utc_timestamp = datetime.datetime.utcnow().replace(
        tzinfo=datetime.timezone.utc).isoformat()

    if mytimer.past_due:
        logging.info('The timer is past due!')

    logging.info('Python timer trigger function ran at %s', utc_timestamp)

    # Add the logic you want to execute here
4. Set the Schedule:
The schedule is set in the function's function.json file using a cron expression.
Example for every 2 hours: "schedule": "0 0 */2 * * *"
5. Deploy the Function to Azure:
Once your function is ready, deploy it to Azure Functions.
You can deploy directly from Visual Studio Code, or use the Azure Functions Core Tools.
6. Integrate with Your Workflow:
This function can now act as a part of your larger workflow, triggering at the scheduled times.
You might have it call your main application logic, perhaps through an HTTP request or by triggering another Azure service.
This setup allows you to schedule and run tasks without needing to manage the underlying infrastructure. The Timer Trigger function in Azure Functions acts independently but can be a part of your broader application workflow.

Let me know if you need additional details on any of these steps or further assistance!




# Import necessary libraries
from azure.cosmos import CosmosClient, exceptions
from pyspark.sql import SparkSession
import requests
import json
import os
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)

# ----- Environment Variable Validation -----
def validate_env_vars(*vars):
    for var in vars:
        if not os.getenv(var):
            logging.error(f"Environment variable {var} is not set.")
            raise EnvironmentError(f"Environment variable {var} is not set.")

# ----- Cosmos DB Connection and Configuration Retrieval -----
def get_cosmosdb_client():
    validate_env_vars('COSMOS_DB_URI', 'COSMOS_DB_KEY')
    url = os.getenv('COSMOS_DB_URI')
    key = os.getenv('COSMOS_DB_KEY')
    return CosmosClient(url, credential=key)

def get_cosmosdb_config(client, database_name, container_name):
    try:
        database = client.get_database_client(database_name)
        container = database.get_container_client(container_name)

        query = "SELECT * FROM c WHERE c.id = 'config_id'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        return items[0] if items else None
    except exceptions.CosmosException as e:
        logging.error(f"Error accessing Cosmos DB: {e}")
        return None

# ----- State Management in Cosmos DB -----
def get_previous_state(container):
    state_document_id = 'stateDocumentId'
    try:
        return container.read_item(item=state_document_id, partition_key=state_document_id)
    except exceptions.CosmosResourceNotFoundError:
        return None

def update_state_in_cosmosdb(container, new_count, new_schema, new_timestamp):
    state_document_id = 'stateDocumentId'
    try:
        state_document = container.read_item(item=state_document_id, partition_key=state_document_id)
    except exceptions.CosmosResourceNotFoundError:
        state_document = {"id": state_document_id}
    
    state_document['lastCount'] = new_count
    state_document['lastSchema'] = new_schema
    state_document['lastCheckedTimestamp'] = new_timestamp
    container.upsert_item(state_document)

# ----- Delta Lake Table Connection -----
def connect_to_delta_lake(delta_table_path):
    spark = SparkSession.builder.appName("DeltaLakeConnection").getOrCreate()
    return spark.read.format("delta").load(delta_table_path)

# ----- Change Data Feed Reading -----
def read_change_data_feed(delta_table_path):
    spark = SparkSession.builder.appName("ReadChangeData").getOrCreate()
    return spark.read.format("delta").option("readChangeData", "true").table(delta_table_path)

# ----- Schema Comparison -----
def compare_schemas(current_schema, stored_schema):
    return str(current_schema) != str(stored_schema)

# ----- Databricks Job Triggering -----
def trigger_databricks_job(databricks_instance, job_id):
    validate_env_vars('DATABRICKS_TOKEN')
    databricks_token = os.getenv('DATABRICKS_TOKEN')

    try:
        url = f"{databricks_instance}/api/2.0/jobs/run-now"
        headers = {
            'Authorization': f'Bearer {databricks_token}',
            'Content-Type': 'application/json'
        }
        payload = {"job_id": job_id}

        response = requests.post(url, headers=headers, data=json.dumps(payload))
        if response.status_code == 200:
            logging.info("Databricks job triggered successfully.")
            return response.json()
        else:
            logging.error(f"Failed to trigger Databricks job: {response.text}")
            return None
    except requests.exceptions.RequestException as e:
        logging.error(f"Error triggering Databricks job: {e}")
        return None

# ----- Main Application Flow -----
def main():
    # Initialize Cosmos DB client
    cosmos_client = get_cosmosdb_client()
    validate_env_vars('COSMOS_DB_DATABASE_NAME', 'COSMOS_DB_CONFIG_CONTAINER_NAME', 'COSMOS_DB_STATE_CONTAINER_NAME')
    database_name = os.getenv('COSMOS_DB_DATABASE_NAME')
    config_container_name = os.getenv('COSMOS_DB_CONFIG_CONTAINER_NAME')
    state_container_name = os.getenv('COSMOS_DB_STATE_CONTAINER_NAME')

    # Retrieve configuration and state containers from Cosmos DB
    config_container = cosmos_client.get_database_client(database_name).get_container_client(config_container_name)
    config = get_cosmosdb_config(cosmos_client, database_name, config_container_name)
    
    state_container = cosmos_client.get_database_client(database_name).get_container_client(state_container_name)
    previous_state = get_previous_state(state_container)

    # Connect to Delta Lake Table
    delta_table_path = config['deltaLakeConfig']['tablePath']
    delta_df = connect_to_delta_lake(delta_table_path)

    # Read Change Data Feed
    change_data = read_change_data_feed(delta_table_path)

    # Check for Schema and Data Changes
    current_count = delta_df.count()
    current_schema = str(delta_df.schema)
    stored_schema = config['deltaLakeConfig']['schema']
    schema_changed = compare_schemas(current_schema, stored_schema)
    data_changed = current_count != (previous_state['lastCount'] if previous_state else 0)

    # Trigger Databricks Job and Update State if Changes Detected
    if schema_changed or data_changed:
        databricks_instance = config['databricksConfig']['instance']
        job_id = config['databricksConfig']['jobId']
        trigger_databricks_job(databricks_instance, job_id)

        current_timestamp = datetime.utcnow().isoformat()
        update_state_in_cosmosdb(state_container, current_count, current_schema, current_timestamp)

if __name__ == "__main__":
    main()




from pyspark.sql import SparkSession

# Ensure that the Delta Lake package is specified in the configuration
# Replace '0.8.0' with the version of the Delta Lake package compatible with your Spark version
spark = SparkSession.builder \
    .appName("DeltaTableExample") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:0.8.0") \
    .getOrCreate()

# If you have the Delta Lake JAR available locally, you could also use the following configuration:
# .config("spark.jars", "/path/to/delta-core_2.12-0.8.0.jar") \

# Specify the path to your Delta table
delta_table_path = "dbfs:/path/to/your/delta/table"

# Read the Delta table
delta_df = spark.read.format("delta").load(delta_table_path)

# Show the first few rows of the DataFrame
delta_df.show()

# Stop the SparkSession when done
spark.stop()


{
    "id": "config_id",
    "deltaTableConfig": {
        "tablePath": "path/to/your/delta/table",
        "schema": "schema_definition_here"
    },
    "databricksJobConfig": {
        "instance": "https://your.databricks.instance",
        "jobId": "your_databricks_job_id"
    }
}


from pyspark.sql import SparkSession

builder = SparkSession.builder.appName("MyApp") \
                      .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                      .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = builder.getOrCreate()






from pyspark.sql import SparkSession

# Configure Spark to use Delta Lake
builder = SparkSession.builder.appName("MyApp") \
                       .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                       .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

# Add the Delta Lake package if you are not using a cluster that already has it
builder = builder.config("spark.jars.packages", "io.delta:delta-core_2.12:<delta_version>")

spark = builder.getOrCreate()



from pyspark.sql import SparkSession

# Initialize a SparkSession with Delta Lake support
spark = SparkSession.builder \
    .appName("DeltaLakeReadExample") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Example path for your Delta table
delta_table_path = "/path/to/your/delta-table"

# Reading data from the Delta table
try:
    delta_df = spark.read.format("delta").load(delta_table_path)

    # Showing the data
    print("Data read successfully from the Delta table:")
    delta_df.show()
except Exception as e:
    print("Error reading the Delta table:", e)

# Stop the SparkSession
spark.stop()





from pyspark.sql import SparkSession

def create_spark_session():
    # Build a SparkSession; only required if not using the provided 'spark' session in Databricks
    spark = SparkSession.builder \
        .appName("DeltaLakeChangeDataFeedExample") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    return spark

def connect_to_delta_lake(spark, delta_table_path):
    # Read the Delta table
    df = spark.read.format("delta").load(delta_table_path)
    df.show(5, False)
    df.printSchema()
    return df

def read_change_data_feed(spark, delta_table_path):
    # Read the change data feed for a Delta table
    change_df = spark.read.format("delta").option("readChangeFeed", "true").table(delta_table_path)
    change_df.show(2, False)
    print("The changed data count:", change_df.count())
    return change_df

def main():
    # Initialize Spark session
    spark = create_spark_session()

    # Example Delta table path - replace with your actual table path
    delta_table_path = "/path/to/your/delta/table"

    # Connect to Delta Lake Table
    connect_to_delta_lake(spark, delta_table_path)

    # Read Change Data Feed
    read_change_data_feed(spark, delta_table_path)

    # Stop the Spark session
    spark.stop()

if __name__ == "__main__":
    main()
