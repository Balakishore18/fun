# Import necessary libraries
from azure.cosmos import CosmosClient
from pyspark.sql import SparkSession
import requests
import json
import os

# ----- Cosmos DB Connection and Configuration Retrieval -----
def get_cosmosdb_config():
    url = os.getenv('COSMOS_DB_URI')
    key = os.getenv('COSMOS_DB_KEY')
    database_name = 'your_database_name'
    container_name = 'your_container_name'

    client = CosmosClient(url, credential=key)
    database = client.get_database_client(database_name)
    container = database.get_container_client(container_name)

    query = "SELECT * FROM c WHERE c.id = 'config_id'"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    return items[0] if items else None

# ----- Delta Lake Table Connection -----
def connect_to_delta_lake(delta_table_path):
    spark = SparkSession.builder.appName("DeltaLakeConnection").getOrCreate()
    df = spark.read.format("delta").load(delta_table_path)
    return df

# ----- Change Data Feed Reading -----
def read_change_data_feed(delta_table_path):
    spark = SparkSession.builder.appName("ReadChangeData").getOrCreate()
    change_df = spark.read.format("delta").option("readChangeData", "true").table(delta_table_path)
    return change_df

# ----- Schema Comparison -----
def compare_schemas(current_schema, stored_schema):
    return str(current_schema) != str(stored_schema)

# ----- Databricks Job Triggering -----
def trigger_databricks_job(databricks_instance, job_id):
    databricks_token = os.getenv('DATABRICKS_TOKEN')
    url = f"{databricks_instance}/api/2.0/jobs/run-now"
    headers = {
        'Authorization': f'Bearer {databricks_token}',
        'Content-Type': 'application/json'
    }
    payload = {"job_id": job_id}

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to trigger job: {response.text}")
        return None

# ----- Main Application Flow -----
def main():
    # Step 1: Retrieve configuration from Cosmos DB
    config = get_cosmosdb_config()
    delta_table_path = config['deltaTableConfig']['tablePath']

    # Step 2: Connect to Delta Lake Table
    delta_df = connect_to_delta_lake(delta_table_path)

    # Step 3: Read Change Data Feed
    change_data = read_change_data_feed(delta_table_path)

    # Step 4: Schema Comparison
    # Assuming 'stored_schema' is retrieved from somewhere (e.g., Cosmos DB)
    stored_schema = config['deltaTableConfig']['schema']
    if compare_schemas(delta_df.schema, stored_schema):
        print("Schema has changed.")

    # Step 5: Trigger Databricks Job
    # Assuming job details are retrieved from the config
    databricks_instance = config['databricksJobConfig']['instance']
    job_id = config['databricksJobConfig']['jobId']
    trigger_response = trigger_databricks_job(databricks_instance, job_id)

    print(f"Databricks job trigger response: {trigger_response}")

if __name__ == "__main__":
    main()


You're right, the consolidated script I provided earlier didn't include the Azure Functions Timer Trigger code. This is because Azure Functions are typically deployed as separate entities in the cloud, and they are not usually part of a standalone Python script. However, I can guide you through creating an Azure Function with a Timer Trigger that would fit into your workflow.

Azure Function with Timer Trigger
Azure Functions with a Timer Trigger are used to execute tasks on a schedule. Hereâ€™s how you can create one:

1. Create an Azure Function App:
You can do this in the Azure portal or using the Azure CLI.
Make sure you have Azure Functions Core Tools installed if you're doing this locally.
2. Create a Function with a Timer Trigger:
Using the Azure Functions Core Tools, you can create a function with a timer trigger using the following command:

bash
Copy code
func new --name MyTimerTrigger --template "Timer trigger" --authlevel anonymous
3. Implement the Function:
This function will be triggered as per the schedule defined in its configuration.

The Python code for the function might look like this:

python
Copy code
import datetime
import logging

import azure.functions as func

def main(mytimer: func.TimerRequest) -> None:
    utc_timestamp = datetime.datetime.utcnow().replace(
        tzinfo=datetime.timezone.utc).isoformat()

    if mytimer.past_due:
        logging.info('The timer is past due!')

    logging.info('Python timer trigger function ran at %s', utc_timestamp)

    # Add the logic you want to execute here
4. Set the Schedule:
The schedule is set in the function's function.json file using a cron expression.
Example for every 2 hours: "schedule": "0 0 */2 * * *"
5. Deploy the Function to Azure:
Once your function is ready, deploy it to Azure Functions.
You can deploy directly from Visual Studio Code, or use the Azure Functions Core Tools.
6. Integrate with Your Workflow:
This function can now act as a part of your larger workflow, triggering at the scheduled times.
You might have it call your main application logic, perhaps through an HTTP request or by triggering another Azure service.
This setup allows you to schedule and run tasks without needing to manage the underlying infrastructure. The Timer Trigger function in Azure Functions acts independently but can be a part of your broader application workflow.

Let me know if you need additional details on any of these steps or further assistance!




# Import necessary libraries
from azure.cosmos import CosmosClient, exceptions
from pyspark.sql import SparkSession
import requests
import json
import os
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)

# ----- Environment Variable Validation -----
def validate_env_vars(*vars):
    for var in vars:
        if not os.getenv(var):
            logging.error(f"Environment variable {var} is not set.")
            raise EnvironmentError(f"Environment variable {var} is not set.")

# ----- Cosmos DB Connection and Configuration Retrieval -----
def get_cosmosdb_client():
    validate_env_vars('COSMOS_DB_URI', 'COSMOS_DB_KEY')
    url = os.getenv('COSMOS_DB_URI')
    key = os.getenv('COSMOS_DB_KEY')
    return CosmosClient(url, credential=key)

def get_cosmosdb_config(client, database_name, container_name):
    try:
        database = client.get_database_client(database_name)
        container = database.get_container_client(container_name)

        query = "SELECT * FROM c WHERE c.id = 'config_id'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        return items[0] if items else None
    except exceptions.CosmosException as e:
        logging.error(f"Error accessing Cosmos DB: {e}")
        return None

# ----- State Management in Cosmos DB -----
def get_previous_state(container):
    state_document_id = 'stateDocumentId'
    try:
        return container.read_item(item=state_document_id, partition_key=state_document_id)
    except exceptions.CosmosResourceNotFoundError:
        return None

def update_state_in_cosmosdb(container, new_count, new_schema, new_timestamp):
    state_document_id = 'stateDocumentId'
    try:
        state_document = container.read_item(item=state_document_id, partition_key=state_document_id)
    except exceptions.CosmosResourceNotFoundError:
        state_document = {"id": state_document_id}
    
    state_document['lastCount'] = new_count
    state_document['lastSchema'] = new_schema
    state_document['lastCheckedTimestamp'] = new_timestamp
    container.upsert_item(state_document)

# ----- Delta Lake Table Connection -----
def connect_to_delta_lake(delta_table_path):
    spark = SparkSession.builder.appName("DeltaLakeConnection").getOrCreate()
    return spark.read.format("delta").load(delta_table_path)

# ----- Change Data Feed Reading -----
def read_change_data_feed(delta_table_path):
    spark = SparkSession.builder.appName("ReadChangeData").getOrCreate()
    return spark.read.format("delta").option("readChangeData", "true").table(delta_table_path)

# ----- Schema Comparison -----
def compare_schemas(current_schema, stored_schema):
    return str(current_schema) != str(stored_schema)

# ----- Databricks Job Triggering -----
def trigger_databricks_job(databricks_instance, job_id):
    validate_env_vars('DATABRICKS_TOKEN')
    databricks_token = os.getenv('DATABRICKS_TOKEN')

    try:
        url = f"{databricks_instance}/api/2.0/jobs/run-now"
        headers = {
            'Authorization': f'Bearer {databricks_token}',
            'Content-Type': 'application/json'
        }a
        payload = {"job_id": job_id}

        response = requests.post(url, headers=headers, data=json.dumps(payload))
        if response.status_code == 200:
            logging.info("Databricks job triggered successfully.")
            return response.json()
        else:
            logging.error(f"Failed to trigger Databricks job: {response.text}")
            return None
    except requests.exceptions.RequestException as e:
        logging.error(f"Error triggering Databricks job: {e}")
        return None

# ----- Main Application Flow -----
def main():
    # Initialize Cosmos DB client
    cosmos_client = get_cosmosdb_client()
    validate_env_vars('COSMOS_DB_DATABASE_NAME', 'COSMOS_DB_CONFIG_CONTAINER_NAME', 'COSMOS_DB_STATE_CONTAINER_NAME')
    database_name = os.getenv('COSMOS_DB_DATABASE_NAME')
    config_container_name = os.getenv('COSMOS_DB_CONFIG_CONTAINER_NAME')
    state_container_name = os.getenv('COSMOS_DB_STATE_CONTAINER_NAME')

    # Retrieve configuration and state containers from Cosmos DB
    config_container = cosmos_client.get_database_client(database_name).get_container_client(config_container_name)
    config = get_cosmosdb_config(cosmos_client, database_name, config_container_name)
    
    state_container = cosmos_client.get_database_client(database_name).get_container_client(state_container_name)
    previous_state = get_previous_state(state_container)

    # Connect to Delta Lake Table
    delta_table_path = config['deltaLakeConfig']['tablePath']
    delta_df = connect_to_delta_lake(delta_table_path)

    # Read Change Data Feed
    change_data = read_change_data_feed(delta_table_path)

    # Check for Schema and Data Changes
    current_count = delta_df.count()
    current_schema = str(delta_df.schema)
    stored_schema = config['deltaLakeConfig']['schema']
    schema_changed = compare_schemas(current_schema, stored_schema)
    data_changed = current_count != (previous_state['lastCount'] if previous_state else 0)

    # Trigger Databricks Job and Update State if Changes Detected
    if schema_changed or data_changed:
        databricks_instance = config['databricksConfig']['instance']
        job_id = config['databricksConfig']['jobId']
        trigger_databricks_job(databricks_instance, job_id)

        current_timestamp = datetime.utcnow().isoformat()
        update_state_in_cosmosdb(state_container, current_count, current_schema, current_timestamp)

if __name__ == "__main__":
    main()




from pyspark.sql import SparkSession

# Ensure that the Delta Lake package is specified in the configuration
# Replace '0.8.0' with the version of the Delta Lake package compatible with your Spark version
spark = SparkSession.builder \
    .appName("DeltaTableExample") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:0.8.0") \
    .getOrCreate()

# If you have the Delta Lake JAR available locally, you could also use the following configuration:
# .config("spark.jars", "/path/to/delta-core_2.12-0.8.0.jar") \

# Specify the path to your Delta table
delta_table_path = "dbfs:/path/to/your/delta/table"

# Read the Delta table
delta_df = spark.read.format("delta").load(delta_table_path)

# Show the first few rows of the DataFrame
delta_df.show()

# Stop the SparkSession when done
spark.stop()


{
    "id": "config_id",
    "deltaTableConfig": {
        "tablePath": "path/to/your/delta/table",
        "schema": "schema_definition_here"
    },
    "databricksJobConfig": {
        "instance": "https://your.databricks.instance",
        "jobId": "your_databricks_job_id"
    }
}


from pyspark.sql import SparkSession

builder = SparkSession.builder.appName("MyApp") \
                      .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                      .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

spark = builder.getOrCreate()






from pyspark.sql import SparkSession

# Configure Spark to use Delta Lake
builder = SparkSession.builder.appName("MyApp") \
                       .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
                       .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

# Add the Delta Lake package if you are not using a cluster that already has it
builder = builder.config("spark.jars.packages", "io.delta:delta-core_2.12:<delta_version>")

spark = builder.getOrCreate()



from pyspark.sql import SparkSession

# Initialize a SparkSession with Delta Lake support
spark = SparkSession.builder \
    .appName("DeltaLakeReadExample") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Example path for your Delta table
delta_table_path = "/path/to/your/delta-table"

# Reading data from the Delta table
try:
    delta_df = spark.read.format("delta").load(delta_table_path)

    # Showing the data
    print("Data read successfully from the Delta table:")
    delta_df.show()
except Exception as e:
    print("Error reading the Delta table:", e)

# Stop the SparkSession
spark.stop()





from pyspark.sql import SparkSession

def create_spark_session():
    # Build a SparkSession; only required if not using the provided 'spark' session in Databricks
    spark = SparkSession.builder \
        .appName("DeltaLakeChangeDataFeedExample") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .getOrCreate()
    return spark

def connect_to_delta_lake(spark, delta_table_path):
    # Read the Delta table
    df = spark.read.format("delta").load(delta_table_path)
    df.show(5, False)
    df.printSchema()
    return df

def read_change_data_feed(spark, delta_table_path):
    # Read the change data feed for a Delta table
    change_df = spark.read.format("delta").option("readChangeFeed", "true").table(delta_table_path)
    change_df.show(2, False)
    print("The changed data count:", change_df.count())
    return change_df

def main():
    # Initialize Spark session
    spark = create_spark_session()

    # Example Delta table path - replace with your actual table path
    delta_table_path = "/path/to/your/delta/table"

    # Connect to Delta Lake Table
    connect_to_delta_lake(spark, delta_table_path)

    # Read Change Data Feed
    read_change_data_feed(spark, delta_table_path)

    # Stop the Spark session
    spark.stop()

if __name__ == "__main__":
    main()




from pyspark.sql import SparkSession

# Function to connect to a Delta table and display the schema and row count
def connect_to_delta_lake(delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        df.show(5, False)
        df.printSchema()
        print(f"Connected successfully to the Delta table. Row count: {df.count()}")
        return df
    except Exception as e:
        print(f"Failed to connect to the Delta table: {e}")
        return None

# Function to read the change data feed from a Delta table
def read_change_data_feed(delta_table_path):
    try:
        change_df = spark.read.format("delta").option("readChangeFeed", "true").table(delta_table_path)
        change_df.show(2, False)  # Show the first 2 changes without truncating the columns
        change_count = change_df.count()  # Get the count of changed rows
        print(f"The changed data count: {change_count}")
        return change_df
    except Exception as e:
        print(f"Failed to read change data feed: {e}")
        return None

# Main function to execute the workflow
def main():
    # Define the path to the Delta table
    delta_table_path = "/path/to/your/delta/table"

    # Step 1: Connect to Delta Lake Table
    print("Connecting to Delta Lake table...")
    df = connect_to_delta_lake(delta_table_path)
    if df is None:
        return  # Exit if connection to Delta table failed

    # Step 2: Read Change Data Feed
    print("Reading change data feed...")
    change_data_df = read_change_data_feed(delta_table_path)
    if change_data_df is None:
        return  # Exit if reading change data feed failed

    # Additional processing with df or change_data_df can be done here
    # ...

# Entry point for the script
if __name__ == "__main__":
    # In Databricks, the Spark session (named 'spark') is already instantiated for you.
    # Hence, you do not need to create it manually.
    # If you run this script outside Databricks, you'll need to uncomment the next two lines:
    
    # spark = SparkSession.builder.appName("DeltaLakeWorkflow").getOrCreate()
    
    main()

    # If outside Databricks, stop the Spark session after the job is done.
    # spark.stop()



from pyspark.sql import SparkSession
from delta.tables import DeltaTable

# This function connects to a Delta table and returns a DataFrame
def connect_to_delta_lake(delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        print("Successfully connected to the Delta table.")
        return df
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None

# This function reads the change data feed from a Delta table using DeltaTable utility
def read_change_data_feed(delta_table):
    try:
        # Assuming the Delta table has the change data feed enabled
        # and the Delta Lake version supports CDF.
        change_df = delta_table.toDF()
        change_df.show(2, False)  # Show 2 rows for brevity
        print("Successfully read change data feed from the Delta table.")
        return change_df
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None

# Main application flow
def main():
    delta_table_path = "/path/to/your/delta/table"  # Specify the path to your Delta table

    # Connect to the Delta table and get a DataFrame
    df = connect_to_delta_lake(delta_table_path)
    if df is None:
        print("Failed to connect to Delta table. Exiting.")
        return

    # Create a DeltaTable instance for the change data feed
    try:
        delta_table = DeltaTable.forPath(spark, delta_table_path)
        print("DeltaTable instance created.")
    except Exception as e:
        print(f"Error creating DeltaTable instance: {e}")
        return

    # Read the change data feed
    change_data_df = read_change_data_feed(delta_table)
    if change_data_df is None:
        print("Failed to read the change data feed. Exiting.")
        return

# Entry point for the script
if __name__ == "__main__":
    # In Databricks, the Spark session is provided as 'spark'.
    # If running outside Databricks, uncomment the next line to create a Spark session.
    # spark = SparkSession.builder.appName("DeltaLakeChangeDataFeed").getOrCreate()

    main()

    # If you created a Spark session, stop it after the application finishes.
    # spark.stop()


from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def connect_to_delta_lake(delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()  # Count the number of rows in the Delta table
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)  # Show the top 2 rows
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def read_change_data_feed(delta_table):
    try:
        # Get the last version of the Delta table
        last_version = delta_table.history(1).select('version').collect()[0][0]
        print(f"Last version of the Delta table: {last_version}")

        # Read the change data feed since the last version
        change_df = delta_table.toDF().where(f"version = {last_version}")
        change_row_count = change_df.count()  # Count the number of rows in the change data feed

        if change_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Successfully read change data feed from the Delta table. Total changed rows: {change_row_count}")
            change_df.show(2, False)  # Show the top 2 changed rows

        return change_df, change_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def main():
    delta_table_path = "/path/to/your/delta/table"  # Specify the path to your Delta table

    # Connect to the Delta table and get a DataFrame
    df, row_count = connect_to_delta_lake(delta_table_path)
    if df is None:
        print("Failed to connect to Delta table. Exiting.")
        return

    # Create a DeltaTable instance for the change data feed
    try:
        delta_table = DeltaTable.forPath(spark, delta_table_path)
        print("DeltaTable instance created.")
    except Exception as e:
        print(f"Error creating DeltaTable instance: {e}")
        return

    # Read the change data feed
    change_data_df, change_row_count = read_change_data_feed(delta_table)
    if change_data_df is None:
        print("Failed to read the change data feed. Exiting.")
        return

# Entry point for the script
if __name__ == "__main__":
    main()

