# Import necessary libraries
from azure.cosmos import CosmosClient
from pyspark.sql import SparkSession
import requests
import json
import os

# ----- Cosmos DB Connection and Configuration Retrieval -----
def get_cosmosdb_config():
    url = os.getenv('COSMOS_DB_URI')
    key = os.getenv('COSMOS_DB_KEY')
    database_name = 'your_database_name'
    container_name = 'your_container_name'

    client = CosmosClient(url, credential=key)
    database = client.get_database_client(database_name)
    container = database.get_container_client(container_name)

    query = "SELECT * FROM c WHERE c.id = 'config_id'"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    return items[0] if items else None

# ----- Delta Lake Table Connection -----
def connect_to_delta_lake(delta_table_path):
    spark = SparkSession.builder.appName("DeltaLakeConnection").getOrCreate()
    df = spark.read.format("delta").load(delta_table_path)
    return df

# ----- Change Data Feed Reading -----
def read_change_data_feed(delta_table_path):
    spark = SparkSession.builder.appName("ReadChangeData").getOrCreate()
    change_df = spark.read.format("delta").option("readChangeData", "true").table(delta_table_path)
    return change_df

# ----- Schema Comparison -----
def compare_schemas(current_schema, stored_schema):
    return str(current_schema) != str(stored_schema)

# ----- Databricks Job Triggering -----
def trigger_databricks_job(databricks_instance, job_id):
    databricks_token = os.getenv('DATABRICKS_TOKEN')
    url = f"{databricks_instance}/api/2.0/jobs/run-now"
    headers = {
        'Authorization': f'Bearer {databricks_token}',
        'Content-Type': 'application/json'
    }
    payload = {"job_id": job_id}

    response = requests.post(url, headers=headers, data=json.dumps(payload))
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to trigger job: {response.text}")
        return None

# ----- Main Application Flow -----
def main():
    # Step 1: Retrieve configuration from Cosmos DB
    config = get_cosmosdb_config()
    delta_table_path = config['deltaTableConfig']['tablePath']

    # Step 2: Connect to Delta Lake Table
    delta_df = connect_to_delta_lake(delta_table_path)

    # Step 3: Read Change Data Feed
    change_data = read_change_data_feed(delta_table_path)

    # Step 4: Schema Comparison
    # Assuming 'stored_schema' is retrieved from somewhere (e.g., Cosmos DB)
    stored_schema = config['deltaTableConfig']['schema']
    if compare_schemas(delta_df.schema, stored_schema):
        print("Schema has changed.")

    # Step 5: Trigger Databricks Job
    # Assuming job details are retrieved from the config
    databricks_instance = config['databricksJobConfig']['instance']
    job_id = config['databricksJobConfig']['jobId']
    trigger_response = trigger_databricks_job(databricks_instance, job_id)

    print(f"Databricks job trigger response: {trigger_response}")

if __name__ == "__main__":
    main()
