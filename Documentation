Introduction
This document serves as a guide for the Self-Service Data Fabric Platform, designed to assist users in setting up and managing their data pipelines, from raw data ingestion to the creation of refined datasets for analytics.

Platform Overview
The Self-Service Data Fabric Platform provides a user-friendly Streamlit interface for users to configure data pipelines, define workflows, and manage data transformations across various layers.

User Interface Guide

App Code: The application's unique identifier.
Sub App Code: Identifier for subdivisions within the application.

Pipeline Configuration
Number of Pipelines: Total pipelines to be set up for the project.
Pipeline Name: Unique name for each pipeline.

Workflow Configuration
Number of Workflows: Total workflows within a pipeline.
Workflow Name: Designation for each workflow.

Ingestion Type: Method of data ingestion.
Data Load Format: Format in which data will be loaded (e.g., CSV, JSON).
Data Load Frequency: How often data is loaded into the system.
Data Storage and Management

After users input their pipeline configurations:
Data Storage: JSON schemas, SQL logic scripts, and raw files are saved in blob storage.

Metadata Table: Manages the overarching details of each pipeline.
A metadata table is created per pipeline, containing:
AppCode: Application code.
SubAppCode: Sub-application code, if applicable.
UniqueID: A unique identifier for each entry.
Workflow Details: Details regarding each workflow.

Configuration Table: Manages workflow-specific details and file references, linked by UniqueID to the metadata table which interm will be used for databricks job creation.
This table includes all details from the UI input, along with references to the associated SQL and JSON files for each layer. It is structured as follows:
UniqueID: Corresponds to a specific workflow and links directly to the metadata table entry for easy reference.
Workflow Details: Configuration specifics for each workflow within the pipeline.
File References: Links to the stored SQL and JSON files in blob storage that pertain to each specific layer.



Summary: The Databricks Job Creation Utility within the Self-Service Data Fabric Platform is an essential automation tool that assists in the setup and execution of data processing jobs on Databricks. Its primary function is to monitor a timestamp table for updates in the metadata table, which contains details of user-entered pipelines. When it finds new entries, marked by timestamps later than the last recorded one, it extracts each entry's unique ID from the metadata table. Following this, it retrieves the related pipeline configurations from the config container, which include SQL scripts and schema details needed to transform raw data into a structured format. The utility then launches Databricks jobs to process the raw data, execute the user-defined SQL transformations, and refresh the Delta tables as designated by the user. Once job creation and data transformation are successfully completed, the utility updates the timestamp table with the newest timestamp, thereby keeping the system up-to-date with the most recent data processing activities.

This automated procedure ensures the conversion of raw data into valuable insights is done according to user-specified transformations and target schemas, thus improving the data pipelineâ€™s efficiency and dependability.



The Python DAG Creator is a straightforward automation tool used with Apache Airflow. It uses configurations stored in a specific container to determine how data jobs should be managed. These configurations include important details like the Databricks job ID and its status. The tool takes this information to build DAGs, which are instructions that tell Airflow how to handle data tasks in Databricks. This involves setting up databases for data storage and applying SQL commands for data processing. The tool ensures that these tasks are prepared and executed accurately, according to the provided settings.


The automated system is designed to initiate Databricks jobs as soon as new data arrives in Blob Storage. It employs an Azure Event Grid Topic to keep an eye on new blob creations within designated storage folders. Each time a blob is added, the Event Grid Topic detects this and notifies an Azure Function. This Azure Function is specifically configured to activate Databricks jobs using the data from the event, such as file details. The setup ensures a quick and automatic transition from data upload to data processing, minimizing the need for manual job initiation.


The system automatically handles the configuration of Databricks jobs when new data is uploaded to Blob Storage. An Azure Event Grid Topic monitors for the creation of new blobs in specific folders. Upon detecting a new blob, it triggers an Azure Function. This function is designed to retrieve the Databricks job configurations based on the blob event details. With this approach, as soon as data is uploaded, the corresponding Databricks job is configured without the need for manual setup, streamlining the data processing workflow.

An Azure Event Grid Topic is set up to monitor the creation of new files in specific Blob Storage folders. When it detects a new file, it triggers an Azure Function. This function is tasked with retrieving the Databricks job configurations based on the details of the blob event. With this system in place, as soon as data is uploaded, the corresponding Databricks job is automatically configured, eliminating the need for manual setup.




I am thinking of two ways to this task


1) If any new data gets inserted in the silver table then I will trigger the gold job

Logic: 

First I will connect to the Databricks delta table capturing the counts based on previous counts if it's less than the current count then I will run the gold job. then I will update the count to the cosmos db table.
To do this I need to change my previous code by adding some function


2) Event-driven approach

Enable Change Data Feed:

In your Delta table on Databricks, enable the change data feed feature by setting the table property delta.enableChangeDataFeed to true. You can do this during table creation or by altering the table after creation.
Use Azure Functions:

Set up an Azure Function with a timer trigger that runs at regular intervals, which is suitable for your use case.
Within this function, write code to query the change feed of your Silver Delta table. Delta Lake on Databricks supports reading the change data feed using Delta Table APIs.
Triggering Logic:

The Azure Function would then compare the timestamps of the changes found in the feed with the last processed timestamp, which you would retrieve from a persistent store such as Azure Table Storage or Azure Cosmos DB.
If there are new changes (i.e., if the latest change timestamp is greater than the last processed timestamp), the function would use the Databricks API to trigger a job. This job would typically process the new changes and update the Gold Delta table accordingly.




