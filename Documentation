Introduction
This document serves as a guide for the Self-Service Data Fabric Platform, designed to assist users in setting up and managing their data pipelines, from raw data ingestion to the creation of refined datasets for analytics.

Platform Overview
The Self-Service Data Fabric Platform provides a user-friendly Streamlit interface for users to configure data pipelines, define workflows, and manage data transformations across various layers.

User Interface Guide

App Code: The application's unique identifier.
Sub App Code: Identifier for subdivisions within the application.

Pipeline Configuration
Number of Pipelines: Total pipelines to be set up for the project.
Pipeline Name: Unique name for each pipeline.

Workflow Configuration
Number of Workflows: Total workflows within a pipeline.
Workflow Name: Designation for each workflow.

Ingestion Type: Method of data ingestion.
Data Load Format: Format in which data will be loaded (e.g., CSV, JSON).
Data Load Frequency: How often data is loaded into the system.
Data Storage and Management

After users input their pipeline configurations:
Data Storage: JSON schemas, SQL logic scripts, and raw files are saved in blob storage.

Metadata Table: Manages the overarching details of each pipeline.
A metadata table is created per pipeline, containing:
AppCode: Application code.
SubAppCode: Sub-application code, if applicable.
UniqueID: A unique identifier for each entry.
Workflow Details: Details regarding each workflow.

Configuration Table: Manages workflow-specific details and file references, linked by UniqueID to the metadata table which interm will be used for databricks job creation.
This table includes all details from the UI input, along with references to the associated SQL and JSON files for each layer. It is structured as follows:
UniqueID: Corresponds to a specific workflow and links directly to the metadata table entry for easy reference.
Workflow Details: Configuration specifics for each workflow within the pipeline.
File References: Links to the stored SQL and JSON files in blob storage that pertain to each specific layer.



Summary: The Databricks Job Creation Utility within the Self-Service Data Fabric Platform is an essential automation tool that assists in the setup and execution of data processing jobs on Databricks. Its primary function is to monitor a timestamp table for updates in the metadata table, which contains details of user-entered pipelines. When it finds new entries, marked by timestamps later than the last recorded one, it extracts each entry's unique ID from the metadata table. Following this, it retrieves the related pipeline configurations from the config container, which include SQL scripts and schema details needed to transform raw data into a structured format. The utility then launches Databricks jobs to process the raw data, execute the user-defined SQL transformations, and refresh the Delta tables as designated by the user. Once job creation and data transformation are successfully completed, the utility updates the timestamp table with the newest timestamp, thereby keeping the system up-to-date with the most recent data processing activities.

This automated procedure ensures the conversion of raw data into valuable insights is done according to user-specified transformations and target schemas, thus improving the data pipelineâ€™s efficiency and dependability.



The Python DAG Creator is a straightforward automation tool used with Apache Airflow. It uses configurations stored in a specific container to determine how data jobs should be managed. These configurations include important details like the Databricks job ID and its status. The tool takes this information to build DAGs, which are instructions that tell Airflow how to handle data tasks in Databricks. This involves setting up databases for data storage and applying SQL commands for data processing. The tool ensures that these tasks are prepared and executed accurately, according to the provided settings.


The automated system is designed to initiate Databricks jobs as soon as new data arrives in Blob Storage. It employs an Azure Event Grid Topic to keep an eye on new blob creations within designated storage folders. Each time a blob is added, the Event Grid Topic detects this and notifies an Azure Function. This Azure Function is specifically configured to activate Databricks jobs using the data from the event, such as file details. The setup ensures a quick and automatic transition from data upload to data processing, minimizing the need for manual job initiation.


The system automatically handles the configuration of Databricks jobs when new data is uploaded to Blob Storage. An Azure Event Grid Topic monitors for the creation of new blobs in specific folders. Upon detecting a new blob, it triggers an Azure Function. This function is designed to retrieve the Databricks job configurations based on the blob event details. With this approach, as soon as data is uploaded, the corresponding Databricks job is configured without the need for manual setup, streamlining the data processing workflow.

An Azure Event Grid Topic is set up to monitor the creation of new files in specific Blob Storage folders. When it detects a new file, it triggers an Azure Function. This function is tasked with retrieving the Databricks job configurations based on the details of the blob event. With this system in place, as soon as data is uploaded, the corresponding Databricks job is automatically configured, eliminating the need for manual setup.




I am thinking of two ways to this task


1) If any new data gets inserted in the silver table then I will trigger the gold job

Logic: 

First I will connect to the Databricks delta table capturing the counts based on previous counts if it's less than the current count then I will run the gold job. then I will update the count to the cosmos db table.
To do this I need to change my previous code by adding some function


2) Event-driven approach

Enable Change Data Feed:

In your Delta table on Databricks, enable the change data feed feature by setting the table property delta.enableChangeDataFeed to true. You can do this during table creation or by altering the table after creation.
Use Azure Functions:

Set up an Azure Function with a timer trigger that runs at regular intervals, which is suitable for your use case.
Within this function, write code to query the change feed of your Silver Delta table. Delta Lake on Databricks supports reading the change data feed using Delta Table APIs.
Triggering Logic:

The Azure Function would then compare the timestamps of the changes found in the feed with the last processed timestamp, which you would retrieve from a persistent store such as Azure Table Storage or Azure Cosmos DB.
If there are new changes (i.e., if the latest change timestamp is greater than the last processed timestamp), the function would use the Databricks API to trigger a job. This job would typically process the new changes and update the Gold Delta table accordingly.


Subtask 1.1: Define Configuration Schema
Determine the structure of the configuration data stored in Cosmos DB (e.g., fields for schedule, Delta table paths, Databricks job details).
Subtask 1.2: Cosmos DB Setup
Create and configure the Cosmos DB instance and container to store the configuration data.
Subtask 1.3: Cosmos DB Connectivity
Implement methods to connect to Cosmos DB and retrieve configuration data using the Azure Cosmos DB SDK for Python.
2. Scheduler Setup
Subtask 2.1: Determine Scheduling Mechanism
Choose a scheduling method (e.g., Azure Functions Timer Trigger, cron job).
Subtask 2.2: Implement Scheduler
Write the code or set up the configuration for the chosen scheduling method.
3. Connect to Delta Lake Table
Subtask 3.1: Access Setup
Set up access to the Delta Lake table, including authentication and necessary permissions.
Subtask 3.2: Delta Table Connection
Write code to establish a connection to the Delta Lake table using appropriate APIs.
4. Data Change Detection
Subtask 4.1: Enable Change Data Feed
Ensure the Delta Lake table has Change Data Feed enabled and understand how to read it.
Subtask 4.2: Read Change Data Feed
Implement logic to read the Change Data Feed and detect new changes since the last check.
5. Schema Change Detection
Subtask 5.1: Schema Retrieval
Implement logic to retrieve the current schema of the Delta Lake table.
Subtask 5.2: Compare Schemas
Write code to compare the retrieved schema with the stored schema to detect changes.
6. Trigger Databricks Job
Subtask 6.1: Databricks API Integration
Integrate with the Databricks API to programmatically trigger jobs.
Subtask 6.2: Job Trigger Logic
Develop the logic that triggers the Databricks job when data or schema changes are detected.
7. State Management
Subtask 7.1: Store Last Checked State in Cosmos DB
Decide on the structure for storing the last checked timestamp and schema in Cosmos DB.
Subtask 7.2: Update State After Check
Implement the update mechanism for the last checked state after each detection cycle.
8. Testing and Validation
Subtask 8.1: Unit Testing
Write unit tests for each component (Cosmos DB retrieval, Delta Lake connection, change detection, etc.).
Subtask 8.2: Integration Testing
Test the entire workflow from configuration retrieval to job triggering to ensure all parts work together.
9. Deployment
Subtask 9.1: Deployment Script
Create scripts or define steps for deploying the solution to the production environment.
Subtask 9.2: Monitoring and Alerts
Set up monitoring and alerts for the solution to notify of successes, failures, or performance issues.
10. Documentation
Subtask 10.1: Code Documentation
Document the codebase, explaining how each part of the system works, including interactions with Cosmos DB.
Subtask 10.2: User Documentation
Write user-facing documentation on how to set up configurations in Cosmos DB, how to deploy, and how to monitor the system.
11. Maintenance and Operations
Subtask 11.1: Define Maintenance Procedures
Outline procedures for regular maintenance, updates, and checks of the system, including the Cosmos DB configurations.
Subtask 11.2: Operational Playbook
Develop an operational playbook for the system, detailing regular operations, incident response, and escalation paths, with a focus on the Cosmos DB component.
Each subtask would be fleshed out with the specifics of the Cosmos DB SDK and how it's used to interact with the configurations needed for your Delta Lake and Databricks setup. Remember to follow Azure's best practices for security and manage your Cosmos DB access keys and credentials securely, preferably using Azure Key Vault or managed identities where possible.



