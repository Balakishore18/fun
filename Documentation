Introduction
This document serves as a guide for the Self-Service Data Fabric Platform, designed to assist users in setting up and managing their data pipelines, from raw data ingestion to the creation of refined datasets for analytics.

Platform Overview
The Self-Service Data Fabric Platform provides a user-friendly Streamlit interface for users to configure data pipelines, define workflows, and manage data transformations across various layers.

User Interface Guide

App Code: The application's unique identifier.
Sub App Code: Identifier for subdivisions within the application.

Pipeline Configuration
Number of Pipelines: Total pipelines to be set up for the project.
Pipeline Name: Unique name for each pipeline.

Workflow Configuration
Number of Workflows: Total workflows within a pipeline.
Workflow Name: Designation for each workflow.

Ingestion Type: Method of data ingestion.
Data Load Format: Format in which data will be loaded (e.g., CSV, JSON).
Data Load Frequency: How often data is loaded into the system.
Data Storage and Management

After users input their pipeline configurations:
Data Storage: JSON schemas, SQL logic scripts, and raw files are saved in blob storage.

Metadata Table: Manages the overarching details of each pipeline.
A metadata table is created per pipeline, containing:
AppCode: Application code.
SubAppCode: Sub-application code, if applicable.
UniqueID: A unique identifier for each entry.
Workflow Details: Details regarding each workflow.

Configuration Table: Manages workflow-specific details and file references, linked by UniqueID to the metadata table which interm will be used for databricks job creation.
This table includes all details from the UI input, along with references to the associated SQL and JSON files for each layer. It is structured as follows:
UniqueID: Corresponds to a specific workflow and links directly to the metadata table entry for easy reference.
Workflow Details: Configuration specifics for each workflow within the pipeline.
File References: Links to the stored SQL and JSON files in blob storage that pertain to each specific layer.



Summary: The Databricks Job Creation Utility within the Self-Service Data Fabric Platform is an essential automation tool that assists in the setup and execution of data processing jobs on Databricks. Its primary function is to monitor a timestamp table for updates in the metadata table, which contains details of user-entered pipelines. When it finds new entries, marked by timestamps later than the last recorded one, it extracts each entry's unique ID from the metadata table. Following this, it retrieves the related pipeline configurations from the config container, which include SQL scripts and schema details needed to transform raw data into a structured format. The utility then launches Databricks jobs to process the raw data, execute the user-defined SQL transformations, and refresh the Delta tables as designated by the user. Once job creation and data transformation are successfully completed, the utility updates the timestamp table with the newest timestamp, thereby keeping the system up-to-date with the most recent data processing activities.

This automated procedure ensures the conversion of raw data into valuable insights is done according to user-specified transformations and target schemas, thus improving the data pipelineâ€™s efficiency and dependability.
