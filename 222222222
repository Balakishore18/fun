def create_and_run_databricks_job(config_data):
    # Convert the config data dictionary to a JSON string
    config_data_json = json.dumps(config_data)

    # Create Databricks job
    create_job_command = ["databricks", "jobs", "create", "--json", config_data_json]
    response = subprocess.run(create_job_command, capture_output=True, text=True)

    if response.returncode != 0:
        raise Exception(f"Error creating job: {response.stderr}")

    # Parse the job ID from the response
    response_json = json.loads(response.stdout)
    job_id = response_json["job_id"]

    # Extract main class name, jar path, and cluster ID from config
    main_class = config_data.get('mainClass')
    jar_path = config_data.get('jarPath')

    for layer, pipelines in config_data['pipelines'].items():
        for pipeline in pipelines:
            run_job_command = [
                "databricks", "jobs", "run-now", "--job-id", str(job_id),
                "--spark-jar-task", {
                    "main_class_name": main_class,
                    "parameters": [pipeline['filePath'], pipeline['filename']],
                    "libraries": [{"jar": jar_path}]
                }
            ]
            response = subprocess.run(run_job_command, capture_output=True, text=True)

            if response.returncode != 0:
                raise Exception(f"Error running job: {response.stderr}")

    print(response.stdout)
