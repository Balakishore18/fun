import sys
import json
import subprocess
from azure.cosmos import CosmosClient, PartitionKey, exceptions

# Cosmos DB Configuration
COSMOS_DB_URL = "YOUR_COSMOS_DB_URL"
COSMOS_DB_KEY = "YOUR_COSMOS_DB_KEY"
COSMOS_DB_NAME = "YOUR_COSMOS_DB_NAME"
COSMOS_DB_CONTAINER_NAME = "YOUR_COSMOS_DB_CONTAINER_NAME"

def get_config_from_cosmodb():
    client = CosmosClient(COSMOS_DB_URL, COSMOS_DB_KEY)
    database = client.get_database_client(COSMOS_DB_NAME)
    container = database.get_container_client(COSMOS_DB_CONTAINER_NAME)

    query = "SELECT * FROM c WHERE c.id = '1'"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    return items[0] if items else None

def create_and_run_databricks_job(config_data):
    job_command = [
        "databricks", 
        "jobs", 
        "create",
        "--name", config_data["name"],
        "--existing-cluster-id", config_data["existing_cluster_id"],
        "--app-code", config_data["appCode"],
        "--sub-app-code", config_data["subAppCode"],
        "--pipeline-name", config_data["pipelineName"],
        "--partition-name", config_data["partitionName"],
        "--main-class-name", config_data["main_class_name"],
        "--jar-uri", config_data["jar_uri"],
        "--tasks-file-path", config_data["taskFilePath"],
        "--config-folder", config_data["configFolder"],
        "--databricks-mode", config_data["databricksMode"],
        "--cosmos-conf-name", config_data["cosmosConfName"],
        "--connection-conf", config_data["CONNECTION_CONF"],
        "--secrets", config_data["secrets"],
        "--config-folder-path", config_data["configFolderPath"]
    ]
    
    response = subprocess.run(job_command, capture_output=True, text=True)

    if response.returncode != 0:
        raise Exception(f"Error creating job: {response.stderr}")

    job_id = json.loads(response.stdout).get('job_id')

    run_job_command = [
        "databricks",
        "jobs",
        "run-now",
        "--job-id", str(job_id),
        "--jar-params",
        config_data["databricksMode"],
        config_data["cosmosConfName"],
        config_data["CONNECTION_CONF"],
        config_data["secrets"],
        config_data["taskFilePath"],
        config_data["configFolder"],
        config_data["jar_uri"],
        config_data["configFolderPath"]
    ]
    
    response = subprocess.run(run_job_command, capture_output=True, text=True)

    if response.returncode != 0:
        raise Exception(f"Error running job: {response.stderr}")

    print(response.stdout)

if __name__ == "__main__":
    config_data = get_config_from_cosmodb()

    if not config_data:
        print("Failed to fetch config data from Cosmos DB.")
        sys.exit(1)

    create_and_run_databricks_job(config_data)





{
    "id": "1",
    "name": "tran_sftp_bronze",
    "existing_cluster_id": "0317-130724-eumsnv4",
    "appCode": "EFF",
    "subAppCode": "D",
    "pipelineName": "transactions",
    "partitionName": "transactions",
    "main_class_name": "com.ssds.cac.engine.exec.FHPPipelineExecutor",
    "jar_uri": "pxo://fh/com.streamsets.ssds.cac.engine.util.0.0.5.jar",
    "taskFilePath": "cust_trans_bronze.json",
    "configFolder": "abfs://databricksonepeebibfefdev02.core.windows.net/EFF/C/Transactions/taskflows/bronze",
    "databricksMode": "Y",
    "cosmosConfName": "SNO_DP_XPH",
    "CONNECTION_CONF": "secretScope=sno-00a",
    "secrets": "taskFile=taskFile=trust_trans_bronze.json",
    "configFolderPath": "/databricks/onepeebibfefdev02.fs.core.windows.net/EFF/C/Transactions/taskflows/bronze"
}



import subprocess
import json

# Define job configuration
job_name = "pxo_sftp_bronze"
main_class = "com.test.ssds.calczine.exec.wfPipeLineExecutor"
args = ["some_argument_here", "another_argument"]
taskFilePath = "cut_trans_bronze.json"
configFolder = "dbfs:/databricks/opeebbleffdev02.dfs.core.windows.net/EFF/C/Transactions/taskflows/bronze"
cluster_id = "abs://databricks/opeebbleffdev02.dfs.core.windows.net/EFF/C/Transactions/taskflows/bronze"
jar_path = "pxo_wtf/uber_sparkstreaming_util_0.0.5.jar"

job_config = {
    "name": job_name,
    "existing_cluster_id": "0317-130724-eumsnnw4",
    "spark_jar_task": {
        "main_class_name": main_class,
        "parameters": args,
        "jar_uri": jar_path
    }
}

# Create the Databricks job
job_create_command = f"databricks jobs create --json '{json.dumps(job_config)}'"
try:
    response = subprocess.check_output(job_create_command, shell=True)
    job_id = json.loads(response)["job_id"]
    print(f"Job created successfully with ID: {job_id}.")

    # Schedule the job to run at a specific time
    # The following cron expression represents every day at 5 PM. Change it to your requirement.
    cron_expression = "0 17 * * *"
    schedule_command = f"databricks jobs runs-now --job-id {job_id}"
    subprocess.run(schedule_command, shell=True, check=True)
    print(f"Job {job_id} scheduled to run at {cron_expression}.")

except subprocess.CalledProcessError as e:
    print(f"Error: {e}")


job_create_command = f'databricks jobs create --json \'{{"name": "{job_name}", "existing_cluster_id": "{job_config["existing_cluster_id"]}", "spark_jar_task": {json.dumps(job_config["spark_jar_task"])} }}\''
try:
    result = subprocess.run(job_create_command, shell=True, check=True, capture_output=True)
    job_id = json.loads(result.stdout)["job_id"]
    print(f"Job (Job ID: {job_id}) created successfully with the name '{job_name}'")
    # Running the job immediately after creation
    run_command = f"databricks jobs run-now --job-id {job_id}"
    subprocess.run(run_command, shell=True, check=True)
    print(f"Job (Job ID: {job_id}) started successfully.")
except subprocess.CalledProcessError as e:
    print(f"Error creating job: {e}")




import requests
import json

# Constants for CosmosDB
COSMOS_DB_ENDPOINT = "YOUR_COSMOS_DB_ENDPOINT"
AUTH_TOKEN = "YOUR_AUTH_TOKEN"

def fetch_config_from_cosmosdb():
    """
    Fetch configuration from CosmosDB.
    """
    headers = {"Authorization": AUTH_TOKEN}
    response = requests.get(COSMOS_DB_ENDPOINT, headers=headers)
    
    if response.status_code != 200:
        raise Exception("Failed to fetch configuration from CosmosDB.")
    
    return response.json()

def get_parameters(pipeline_name, layer_name, config_data):
    """
    Retrieve specific parameters from the configuration.
    """
    return config_data.get(pipeline_name, {}).get(layer_name, {})

def initiate_job_based_on_params(params):
    """
    Initiate a job based on the provided parameters. This function is a placeholder.
    You can add more logic to this function based on the specific actions you want to perform with the parameters.
    """
    print(f"Initiating job for {params['pipelineName']} - {params['layer']}")
    print(f"Filename: {params['filename']}")
    print(f"Filepath: {params['filePath']}")
    # Additional logic here

def main():
    # Fetch configuration data
    config_data = fetch_config_from_cosmosdb()
    
    # Example of fetching parameters for the 'Information' pipeline and 'bronze' layer
    bronze_params = get_parameters("Information", "bronze", config_data)
    
    # Initiate a job based on the bronze parameters
    initiate_job_based_on_params(bronze_params)

if __name__ == "__main__":
    main()

