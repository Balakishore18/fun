import json
import subprocess
from azure.cosmos import CosmosClient, PartitionKey, exceptions

# CosmosDB Configuration
url = 'your_cosmosdb_url'
key = 'your_primary_key'
client = CosmosClient(url, credential=key)
database = client.get_database_client('your_database_name')
container = database.get_container_client('your_container_name')

def get_config_from_cosmosdb():
    items = list(container.query_items(query="SELECT * FROM C", enable_cross_partition_query=True))
    return items[0] if items else None  # Assuming only one config document

def create_and_run_databricks_job(config_data):
    # Extract main class name, jar path, and cluster ID from config
    main_class = config_data.get('mainClass')
    jar_path = config_data.get('jarPath')
    existing_cluster_id = config_data.get('existingClusterID')
    
    for layer, pipelines in config_data['pipelines'].items():
        for pipeline in pipelines:
            cmd = f"""
            databricks jobs create --json '{{
                "name": "{pipeline['pipelineName']}_{pipeline['layer']}",
                "existing_cluster_id": "{existing_cluster_id}",
                "spark_jar_task": {{
                    "main_class_name": "{main_class}",
                    "parameters": ["{pipeline['filePath']}"],
                    "libraries": [{{
                        "jar": "{jar_path}"
                    }}]
                }}
            }}'
            """
            subprocess.run(cmd, shell=True)

if __name__ == "__main__":
    config_data = get_config_from_cosmosdb()
    if config_data:
        create_and_run_databricks_job(config_data)
    else:
        print("Error: No configuration data found in CosmosDB.")




{
    "mainClass": "com.yourcompany.MainClassName",
    "jarPath": "dbfs:/path_to_your_jar/your_jar_name.jar",
    "existingClusterID": "your_existing_cluster_id",
    "pipelines": {
        "bronze": [
            {
                "pipelineName": "transactions",
                "layer": "bronze",
                "filename": "cust_info_bronze.json",
                "filePath": "EFF/D/transactions/taskflows/bronze/cust_info_bronze.json"
            }
        ],
        "silver": [
            {
                "pipelineName": "transactions",
                "layer": "silver",
                "filename": "cust_trans_silver.json",
                "filePath": "EFF/D/transactions/taskflows/silver/cust_trans_silver.json"
            },
            {
                "pipelineName": "transactions",
                "layer": "silver",
                "filename": "DF_dedupeTables.sql",
                "filePath": "EFF/D/transactions/sql/DF_dedupeTables.sql"
            }
        ]
    }
}
