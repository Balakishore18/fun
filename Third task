import paramiko

# Set up the connection parameters based on your server.
sftp_host = "your.sftp.host"
sftp_port = 22  # or your custom port
sftp_username = "your_username"
sftp_password = "your_password"

# Initialize the SSH client
client = paramiko.SSHClient()

# Automatically add the server's SSH key (unsafe for production, see below)
client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

try:
    # Connect to the SFTP server
    client.connect(sftp_host, port=sftp_port, username=sftp_username, password=sftp_password)
    
    # Open an SFTP session on the SSH server
    sftp = client.open_sftp()
    
    # Navigate to the directory and list files
    sftp.chdir('your/directory/path')
    for filename in sftp.listdir():
        print(filename)
    
    # Close the SFTP session
    sftp.close()

except Exception as e:
    print(f"An error occurred: {e}")

finally:
    # Close the client connection
    client.close()




import pysftp

cnopts = pysftp.CnOpts()
cnopts.hostkeys = None  # Use with caution, disables host key verification

# Define the connection parameters
sftp_host = 'your.sftp.host'
sftp_username = 'your_username'
sftp_password = 'your_password'
remote_path = '/path/to/directory'

# Connect to the SFTP server
with pysftp.Connection(host=sftp_host, username=sftp_username, password=sftp_password, cnopts=cnopts) as sftp:
    # Switch to the remote directory
    sftp.cwd(remote_path)
    
    # List the files in the current directory on the SFTP server
    directory_structure = sftp.listdir_attr()
    
    for attr in directory_structure:
        print(f"Filename: {attr.filename}, Size: {attr.st_size}, Last modified: {attr.st_mtime}")

# Checking for new or modified files would require keeping a record of previous states to compare against.



import paramiko

# Define the connection parameters
sftp_host = 'your.sftp.host'  # Replace with your SFTP host
sftp_port = 22  # Replace with your SFTP port if different
sftp_username = 'your_username'  # Replace with your SFTP username
sftp_password = 'your_password'  # Replace with your SFTP password

# Initialize the SSH client
client = paramiko.SSHClient()

# For security, we use the system host keys. You can also use AutoAddPolicy to accept unknown host keys (not recommended).
client.load_system_host_keys()
client.set_missing_host_key_policy(paramiko.WarningPolicy())

try:
    # Connect to the SFTP server
    client.connect(sftp_host, port=sftp_port, username=sftp_username, password=sftp_password)
    
    # Open an SFTP session on the SSH server
    sftp = client.open_sftp()
    
    # List the files in the current directory on the SFTP server
    print("Listing files in the home directory:")
    for filename in sftp.listdir('.'):
        print(filename)
    
    # Close the SFTP session
    sftp.close()

except paramiko.AuthenticationException:
    print("Authentication failed, please verify your credentials")
except paramiko.SSHException as e:
    print(f"SSH error: {e}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")
finally:
    # Close the client connection
    client.close()
import requests
from requests.auth import HTTPBasicAuth

def get_airflow_data():
    # Replace with your Airflow credentials
    airflow_username = "your_username"
    airflow_password = "your_password"
    
    # Replace with your Airflow API endpoint
    api_endpoint = "http://your-airflow-api-endpoint/api/v1/dags"

    try:
        # Make GET request with basic authentication
        response = requests.get(api_endpoint, auth=HTTPBasicAuth(airflow_username, airflow_password))

        # Check if the request was successful (status code 200)
        if response.status_code == 200:
            data = response.json()
            # Process the data as needed
            print("Airflow API Data:")
            print(data)
        else:
            print(f"Error: {response.status_code} - {response.text}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Call the function to get Airflow data
get_airflow_data()


import requests
from requests.auth import HTTPBasicAuth
from azure.cosmos import CosmosClient

def get_airflow_data():
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoints for DAGs and DAG Runs
    dag_api_endpoint = "http://your-airflow-api-endpoint/api/v1/dags"
    dag_runs_api_endpoint = "http://your-airflow-api-endpoint/api/v1/dagRuns"

    # Replace with your Cosmos DB connection details
    cosmos_db_endpoint = "https://your-cosmos-db-name.documents.azure.com:443/"
    cosmos_db_key = "your_cosmos_db_key"
    cosmos_db_database_id = "your_database_id"
    cosmos_db_container_id = "your_container_id"

    # Initialize Cosmos DB client
    cosmos_client = CosmosClient(cosmos_db_endpoint, cosmos_db_key)
    database = cosmos_client.get_database_client(cosmos_db_database_id)
    container = database.get_container_client(cosmos_db_container_id)

    try:
        # Make GET request to Airflow API to get DAGs
        dag_response = requests.get(dag_api_endpoint, auth=HTTPBasicAuth(airflow_username, airflow_password))
        dags = dag_response.json()

        for dag in dags:
            dag_id = dag["dag_id"]

            # Make GET request to Airflow API to get DAG information
            dag_info_response = requests.get(f"{dag_api_endpoint}/{dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
            dag_info = dag_info_response.json()

            # Make GET request to Airflow API to get DAG runs
            dag_runs_response = requests.get(f"{dag_runs_api_endpoint}/{dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
            dag_runs = dag_runs_response.json()

            # Assuming you want to store the data in Cosmos DB
            # Replace this with your specific data model and container logic
            item = {
                "dag_id": dag_id,
                "dag_info": dag_info,
                "dag_runs": dag_runs
            }

            # Insert the item into Cosmos DB container
            container.upsert_item(item)

        print("Data successfully retrieved and stored in Cosmos DB.")

    except Exception as e:
        print(f"An error occurred: {e}")

# Call the function to get Airflow data and store in Cosmos DB
get_airflow_data()



import requests
from requests.auth import HTTPBasicAuth
from azure.cosmos import CosmosClient

def get_and_store_dag_data(manual_dag_name):
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoints for DAGs and DAG Runs
    dag_api_endpoint = "http://your-airflow-api-endpoint/api/v1/dags"
    dag_runs_api_endpoint = "http://your-airflow-api-endpoint/api/v1/dagRuns"

    # Replace with your Cosmos DB connection details
    cosmos_db_endpoint = "https://your-cosmos-db-name.documents.azure.com:443/"
    cosmos_db_key = "your_cosmos_db_key"
    cosmos_db_database_id = "your_database_id"
    cosmos_db_container_id = "your_container_id"

    # Initialize Cosmos DB client
    cosmos_client = CosmosClient(cosmos_db_endpoint, cosmos_db_key)
    database = cosmos_client.get_database_client(cosmos_db_database_id)
    container = database.get_container_client(cosmos_db_container_id)

    try:
        # Specify the DAG manually
        manual_dag_id = manual_dag_name

        # Make GET request to Airflow API to get DAG information
        dag_info_response = requests.get(f"{dag_api_endpoint}/{manual_dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_info = dag_info_response.json()

        # Make GET request to Airflow API to get DAG runs
        dag_runs_response = requests.get(f"{dag_runs_api_endpoint}/{manual_dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_runs = dag_runs_response.json()

        # Assuming you want to store the data in Cosmos DB
        # Replace this with your specific data model and container logic
        item = {
            "dag_id": manual_dag_id,
            "dag_info": dag_info,
            "dag_runs": dag_runs
        }

        # Insert the item into Cosmos DB container
        container.upsert_item(item)

        print(f"Data for DAG '{manual_dag_id}' successfully retrieved and stored in Cosmos DB.")

    except Exception as e:
        print(f"An error occurred: {e}")

# Specify the DAG name manually
dag_name_to_retrieve = "your_manual_dag_name"

# Call the function to get Airflow data for the specified DAG and store in Cosmos DB
get_and_store_dag_data(dag_name_to_retrieve)
