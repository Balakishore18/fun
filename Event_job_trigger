import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def connect_to_delta_lake(delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def get_last_processed_version(delta_table):
    try:
        # Implement the logic to retrieve the last processed version
        # For example, read from a file or a database
        last_processed_version = # Your logic here
        return last_processed_version
    except Exception as e:
        print(f"Error getting the last processed version: {e}")
        return None

def update_last_processed_version(delta_table, new_version):
    try:
        # Implement the logic to update the last processed version
        # For example, write to a file or a database
        # Your logic here
    except Exception as e:
        print(f"Error updating the last processed version: {e}")

def read_change_data_feed(spark, delta_table_path, previous_version):
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(job_id):
    domain = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
    url = f"https://{domain}/api/2.0/jobs/run-now"
    
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    delta_table_path = "/path/to/your/delta/table"
    job_id = "<your-job-id>"  # Replace with your actual job ID

    df, row_count = connect_to_delta_lake(delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)
    last_processed_version = get_last_processed_version(delta_table)

    if latest_version is not None and last_processed_version is not None and latest_version > last_processed_version:
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, last_processed_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(job_id)
            update_last_processed_version(delta_table, latest_version)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("No new changes since the last processed version or unable to determine versions.")

if __name__ == "__main__":
    main()




import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def create_spark_session(databricks_workspace, databricks_token):
    """
    Create and configure a Spark session to connect to Databricks.
    """
    spark_builder = SparkSession.builder.appName("DeltaLakeChangeDetection")
    spark_builder.config("spark.databricks.workspaceUrl", databricks_workspace)
    spark_builder.config("spark.databricks.token", databricks_token)
    spark_builder.config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    spark_builder.config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

    # Add any other necessary configuration specific to your Databricks setup
    return spark_builder.getOrCreate()

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    """
    Trigger a Databricks job using the REST API and PAT for authentication.
    """
    url = f"https://{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

# Other functions (connect_to_delta_lake, get_latest_version, read_change_data_feed) remain the same

def main():
    databricks_workspace = "<your-databricks-workspace-url>"  # e.g., "dbc-1234.cloud.databricks.com"
    databricks_token = "<your-databricks-pat-token>"
    delta_table_path = "/path/to/your/delta/table"
    job_id = "<your-job-id>"  # Replace with your actual job ID

    # Initialize Spark session to connect to Databricks
    spark = create_spark_session(databricks_workspace, databricks_token)

    df, row_count = connect_to_delta_lake(delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)

    if latest_version is not None and latest_version > 0:
        previous_version = latest_version - 1
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, previous_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(databricks_workspace, databricks_token, job_id)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("Could not determine the latest version for the Delta table or no previous version available.")

if __name__ == "__main__":
    main()
