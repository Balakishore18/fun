import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def connect_to_delta_lake(delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def get_last_processed_version(delta_table):
    try:
        # Implement the logic to retrieve the last processed version
        # For example, read from a file or a database
        last_processed_version = # Your logic here
        return last_processed_version
    except Exception as e:
        print(f"Error getting the last processed version: {e}")
        return None

def update_last_processed_version(delta_table, new_version):
    try:
        # Implement the logic to update the last processed version
        # For example, write to a file or a database
        # Your logic here
    except Exception as e:
        print(f"Error updating the last processed version: {e}")

def read_change_data_feed(spark, delta_table_path, previous_version):
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(job_id):
    domain = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
    url = f"https://{domain}/api/2.0/jobs/run-now"
    
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    delta_table_path = "/path/to/your/delta/table"
    job_id = "<your-job-id>"  # Replace with your actual job ID

    df, row_count = connect_to_delta_lake(delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)
    last_processed_version = get_last_processed_version(delta_table)

    if latest_version is not None and last_processed_version is not None and latest_version > last_processed_version:
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, last_processed_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(job_id)
            update_last_processed_version(delta_table, latest_version)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("No new changes since the last processed version or unable to determine versions.")

if __name__ == "__main__":
    main()




import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def create_spark_session(databricks_workspace, databricks_token):
    """
    Create and configure a Spark session to connect to Databricks.
    """
    spark_builder = SparkSession.builder.appName("DeltaLakeChangeDetection")
    spark_builder.config("spark.databricks.workspaceUrl", databricks_workspace)
    spark_builder.config("spark.databricks.token", databricks_token)
    spark_builder.config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    spark_builder.config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

    # Add any other necessary configuration specific to your Databricks setup
    return spark_builder.getOrCreate()

def connect_to_delta_lake(spark, delta_table_path):
    """
    Connect to a Delta Lake table and return a DataFrame and the row count.
    """
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    """
    Get the latest version of the Delta table.
    """
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    """
    Read the change data feed from the Delta table.
    """
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    """
    Trigger a Databricks job using the REST API and PAT for authentication.
    """
    url = f"https://{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    databricks_workspace = "<your-databricks-workspace-url>"  # e.g., "dbc-1234.cloud.databricks.com"
    databricks_token = "<your-databricks-pat-token>"
    delta_table_path = "/path/to/your/delta/table"
    job_id = "<your-job-id>"  # Replace with your actual job ID

    # Initialize Spark session to connect to Databricks
    spark = create_spark_session(databricks_workspace, databricks_token)

    df, row_count = connect_to_delta_lake(spark, delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)

    if latest_version is not None and latest_version > 0:
        previous_version = latest_version - 1
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, previous_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(databricks_workspace, databricks_token, job_id)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("Could not determine the latest version for the Delta table or no previous version available.")

if __name__ == "__main__":
    main()



{
    "id": "config1",
    "databricks_workspace_url": "dbc-1234.cloud.databricks.com",
    "databricks_pat_token": "your-databricks-pat-token",
    "delta_table_path": "/path/to/your/delta/table",
    "job_id": "your-job-id"
}


import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable
from azure.cosmos import CosmosClient, exceptions

def create_spark_session(databricks_workspace, databricks_token):
    """
    Create and configure a Spark session to connect to Databricks.
    """
    spark_builder = SparkSession.builder.appName("DeltaLakeChangeDetection")
    spark_builder.config("spark.databricks.workspaceUrl", databricks_workspace)
    spark_builder.config("spark.databricks.token", databricks_token)
    spark_builder.config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    spark_builder.config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    return spark_builder.getOrCreate()

def connect_to_delta_lake(spark, delta_table_path):
    """
    Connect to a Delta Lake table and return a DataFrame and the row count.
    """
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    """
    Get the latest version of the Delta table.
    """
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    """
    Read the change data feed from the Delta table.
    """
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    """
    Trigger a Databricks job using the REST API and PAT for authentication.
    """
    url = f"https://{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def load_config_from_cosmos(db_url, db_key, db_name, container_name, config_id):
    """
    Load configuration from Azure Cosmos DB.
    """
    client = CosmosClient(db_url, credential=db_key)
    database = client.get_database_client(db_name)
    container = database.get_container_client(container_name)

    try:
        config_item = container.read_item(item=config_id, partition_key=config_id)
        return config_item
    except exceptions.CosmosResourceNotFoundError:
        print("Configuration item not found in Cosmos DB")
        return None

def main():
    cosmos_db_url = "<your-cosmos-db-url>"
    cosmos_db_key = "<your-cosmos-db-key>"
    cosmos_db_name = "<your-cosmos-db-name>"
    cosmos_container_name = "<your-container-name>"
    config_id = "<your-config-id>"

    config = load_config_from_cosmos(cosmos_db_url, cosmos_db_key, cosmos_db_name, cosmos_container_name, config_id)
    if config is None:
        return

    databricks_workspace = config["databricks_workspace_url"]
    databricks_token = config["databricks_pat_token"]
    delta_table_path = config["delta_table_path"]
    job_id = config["job_id"]

    spark = create_spark_session(databricks_workspace, databricks_token)
    df, row_count = connect_to_delta_lake(spark, delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)

    if latest_version is not None and latest_version > 0:
        previous_version = latest_version - 1
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, previous_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(databricks_workspace, databricks_token, job_id)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("Could not determine the latest version for the Delta table or no previous version available.")

if __name__ == "__main__":
    main()




from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .getOrCreate()


from pyspark.sql import SparkSession

# Replace these with your actual account name and access key
account_name = "your-account-name"
access_key = "your-access-key"

spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("fs.azure.account.key." + account_name + ".dfs.core.windows.net", access_key) \
    .config("fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .getOrCreate()


from pyspark.sql import SparkSession

# Azure Storage account information
account_name = "your_account_name"  # Replace with your storage account name
access_key = "your_access_key"  # Replace with your storage account access key

# Initialize SparkSession with Delta Lake package and Azure Storage configuration
spark = SparkSession.builder \
    .appName("DeltaTableConnector") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config(f"fs.azure.account.key.{account_name}.dfs.core.windows.net", access_key) \
    .config("fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .getOrCreate()

# Path to the Delta table
delta_table_path = "dbfs:/path/to/your/delta/table"  # Replace with the path to your Delta table

# Read the Delta table
df = spark.read.format("delta").load(delta_table_path)

# Show the data
df.show()


