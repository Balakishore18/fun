import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def connect_to_delta_lake(delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def get_last_processed_version(delta_table):
    try:
        # Implement the logic to retrieve the last processed version
        # For example, read from a file or a database
        last_processed_version = # Your logic here
        return last_processed_version
    except Exception as e:
        print(f"Error getting the last processed version: {e}")
        return None

def update_last_processed_version(delta_table, new_version):
    try:
        # Implement the logic to update the last processed version
        # For example, write to a file or a database
        # Your logic here
    except Exception as e:
        print(f"Error updating the last processed version: {e}")

def read_change_data_feed(spark, delta_table_path, previous_version):
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(job_id):
    domain = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get("browserHostName").get()
    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()
    url = f"https://{domain}/api/2.0/jobs/run-now"
    
    headers = {
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    delta_table_path = "/path/to/your/delta/table"
    job_id = "<your-job-id>"  # Replace with your actual job ID

    df, row_count = connect_to_delta_lake(delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)
    last_processed_version = get_last_processed_version(delta_table)

    if latest_version is not None and last_processed_version is not None and latest_version > last_processed_version:
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, last_processed_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(job_id)
            update_last_processed_version(delta_table, latest_version)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("No new changes since the last processed version or unable to determine versions.")

if __name__ == "__main__":
    main()




import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

def create_spark_session(databricks_workspace, databricks_token):
    """
    Create and configure a Spark session to connect to Databricks.
    """
    spark_builder = SparkSession.builder.appName("DeltaLakeChangeDetection")
    spark_builder.config("spark.databricks.workspaceUrl", databricks_workspace)
    spark_builder.config("spark.databricks.token", databricks_token)
    spark_builder.config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    spark_builder.config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

    # Add any other necessary configuration specific to your Databricks setup
    return spark_builder.getOrCreate()

def connect_to_delta_lake(spark, delta_table_path):
    """
    Connect to a Delta Lake table and return a DataFrame and the row count.
    """
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    """
    Get the latest version of the Delta table.
    """
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    """
    Read the change data feed from the Delta table.
    """
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    """
    Trigger a Databricks job using the REST API and PAT for authentication.
    """
    url = f"https://{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    databricks_workspace = "<your-databricks-workspace-url>"  # e.g., "dbc-1234.cloud.databricks.com"
    databricks_token = "<your-databricks-pat-token>"
    delta_table_path = "/path/to/your/delta/table"
    job_id = "<your-job-id>"  # Replace with your actual job ID

    # Initialize Spark session to connect to Databricks
    spark = create_spark_session(databricks_workspace, databricks_token)

    df, row_count = connect_to_delta_lake(spark, delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)

    if latest_version is not None and latest_version > 0:
        previous_version = latest_version - 1
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, previous_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(databricks_workspace, databricks_token, job_id)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("Could not determine the latest version for the Delta table or no previous version available.")

if __name__ == "__main__":
    main()



{
    "id": "config1",
    "databricks_workspace_url": "dbc-1234.cloud.databricks.com",
    "databricks_pat_token": "your-databricks-pat-token",
    "delta_table_path": "/path/to/your/delta/table",
    "job_id": "your-job-id"
}


import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable
from azure.cosmos import CosmosClient, exceptions

def create_spark_session(databricks_workspace, databricks_token):
    """
    Create and configure a Spark session to connect to Databricks.
    """
    spark_builder = SparkSession.builder.appName("DeltaLakeChangeDetection")
    spark_builder.config("spark.databricks.workspaceUrl", databricks_workspace)
    spark_builder.config("spark.databricks.token", databricks_token)
    spark_builder.config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    spark_builder.config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    return spark_builder.getOrCreate()

def connect_to_delta_lake(spark, delta_table_path):
    """
    Connect to a Delta Lake table and return a DataFrame and the row count.
    """
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    """
    Get the latest version of the Delta table.
    """
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    """
    Read the change data feed from the Delta table.
    """
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()

        if changes_row_count == 0:
            print("No changes detected since the last version.")
        else:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)

        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    """
    Trigger a Databricks job using the REST API and PAT for authentication.
    """
    url = f"https://{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Triggered job {job_id}. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def load_config_from_cosmos(db_url, db_key, db_name, container_name, config_id):
    """
    Load configuration from Azure Cosmos DB.
    """
    client = CosmosClient(db_url, credential=db_key)
    database = client.get_database_client(db_name)
    container = database.get_container_client(container_name)

    try:
        config_item = container.read_item(item=config_id, partition_key=config_id)
        return config_item
    except exceptions.CosmosResourceNotFoundError:
        print("Configuration item not found in Cosmos DB")
        return None

def main():
    cosmos_db_url = "<your-cosmos-db-url>"
    cosmos_db_key = "<your-cosmos-db-key>"
    cosmos_db_name = "<your-cosmos-db-name>"
    cosmos_container_name = "<your-container-name>"
    config_id = "<your-config-id>"

    config = load_config_from_cosmos(cosmos_db_url, cosmos_db_key, cosmos_db_name, cosmos_container_name, config_id)
    if config is None:
        return

    databricks_workspace = config["databricks_workspace_url"]
    databricks_token = config["databricks_pat_token"]
    delta_table_path = config["delta_table_path"]
    job_id = config["job_id"]

    spark = create_spark_session(databricks_workspace, databricks_token)
    df, row_count = connect_to_delta_lake(spark, delta_table_path)
    if df is None:
        print("Failed to connect to the Delta table. Exiting.")
        return

    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)

    if latest_version is not None and latest_version > 0:
        previous_version = latest_version - 1
        change_data_df, change_row_count = read_change_data_feed(spark, delta_table_path, previous_version)
        if change_row_count > 0:
            print(f"Changes detected. Triggering job with ID {job_id}.")
            trigger_databricks_job(databricks_workspace, databricks_token, job_id)
        else:
            print("No changes detected. No job will be triggered.")
    else:
        print("Could not determine the latest version for the Delta table or no previous version available.")

if __name__ == "__main__":
    main()




from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .getOrCreate()


from pyspark.sql import SparkSession

# Replace these with your actual account name and access key
account_name = "your-account-name"
access_key = "your-access-key"

spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("fs.azure.account.key." + account_name + ".dfs.core.windows.net", access_key) \
    .config("fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .getOrCreate()


from pyspark.sql import SparkSession

# Azure Storage account information
account_name = "your_account_name"  # Replace with your storage account name
access_key = "your_access_key"  # Replace with your storage account access key

# Initialize SparkSession with Delta Lake package and Azure Storage configuration
spark = SparkSession.builder \
    .appName("DeltaTableConnector") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.3.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config(f"fs.azure.account.key.{account_name}.dfs.core.windows.net", access_key) \
    .config("fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statestr.com -Dhttp.proxyPort=80 -Dhttps.proxyHost=proxy.statestr.com -Dhttps.proxyPort=80") \
    .getOrCreate()

# Path to the Delta table
delta_table_path = "dbfs:/path/to/your/delta/table"  # Replace with the path to your Delta table

# Read the Delta table
df = spark.read.format("delta").load(delta_table_path)

# Show the data
df.show()


from pyspark.sql import SparkSession
from delta.tables import DeltaTable
from azure.cosmos import CosmosClient, exceptions

# Initialize Cosmos DB Client
cosmos_endpoint = "YOUR_COSMOS_DB_ENDPOINT"
cosmos_key = "YOUR_COSMOS_DB_KEY"
database_name = "YOUR_DATABASE_NAME"
container_name = "YOUR_CONTAINER_NAME"

client = CosmosClient(cosmos_endpoint, cosmos_key)
database_client = client.get_database_client(database_name)
container_client = database_client.get_container_client(container_name)

# Initialize Spark Session
spark = (
    SparkSession.builder
    .appName("DeltaTableChangeFeed")
    .getOrCreate()
)

# Function to get the last processed timestamp and version from Cosmos DB
def get_last_processed_info():
    try:
        for item in container_client.query_items(
            query="SELECT * FROM c WHERE c.id = 'last_processed_info'",
            enable_cross_partition_query=True):
            return item['timestamp'], item['version']
    except exceptions.CosmosHttpResponseError as e:
        print(f"An error occurred: {e}")
        return None, None

# Function to update the last processed timestamp and version in Cosmos DB
def update_last_processed_info(timestamp, version):
    try:
        container_client.upsert_item({
            'id': 'last_processed_info',
            'timestamp': timestamp,
            'version': version
        })
    except exceptions.CosmosHttpResponseError as e:
        print(f"An error occurred: {e}")

# Function to connect to delta table
def connect_to_delta_table(spark, delta_table_path):
    # ... Your existing logic to connect to the Delta table ...
    pass

# Function to get the latest version of the Delta table
def get_latest_version(delta_table):
    try:
        history_df = delta_table.history(1)
        latest_version = history_df.select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

# Main logic
def main():
    delta_table_path = "YOUR_DELTA_TABLE_PATH"
    delta_table = DeltaTable.forPath(spark, delta_table_path)
    latest_version = get_latest_version(delta_table)  # Your function to get the latest version
    latest_timestamp = delta_table.history(1).select("timestamp").collect()[0]["timestamp"]

    last_processed_timestamp, last_processed_version = get_last_processed_info()

    if latest_version is not None and latest_timestamp is not None:
        if (latest_version > last_processed_version or 
            latest_timestamp > last_processed_timestamp):
            # Trigger the job only if there are new changes
            print(f"New changes detected. Triggering job...")
            # Your code to trigger the Databricks job
            
            # Update the last processed timestamp and version
            update_last_processed_info(latest_timestamp, latest_version)
        else:
            print("No new changes since the last run. Job will not be triggered.")
    else:
        print("Unable to determine the latest version or timestamp of the delta table.")

# Entry point for the script
if __name__ == "__main__":
    main()




from pyspark.sql.functions import col

def has_schema_changed(spark, delta_table_path, last_processed_version):
    # Get the history of the Delta table
    history_df = spark.sql(f"DESCRIBE HISTORY `{delta_table_path}`")
    
    # Get the schema of the last processed version
    last_schema_df = history_df.where(col("version") == last_processed_version).select("operationParameters")

    # Get the schema of the latest version
    latest_schema_df = history_df.orderBy(col("version").desc()).limit(1).select("operationParameters")

    # Check if the schema has changed by comparing the operationParameters
    schema_changed = last_schema_df.collect()[0] != latest_schema_df.collect()[0]
    
    return schema_changed

# Use the function in your main logic to determine if a job should be triggered
if __name__ == "__main__":
    # ... Your existing code for spark session and variables ...

    latest_version = get_latest_version(delta_table_path)
    last_processed_version = get_last_processed_info(tablename)

    # Check for schema changes
    schema_changed = has_schema_changed(spark, delta_table_path, last_processed_version)
    
    if latest_version is not None:
        if (latest_version > last_processed_version) or schema_changed:
            # Trigger job and process schema changes
            print("Changes detected in data or schema. Triggering job...")
            # ... Code to trigger the Databricks job ...
        else:
            print("No changes since the last run. Job will not be triggered.")
    else:
        print("Unable to determine the latest version or timestamp of the delta table.")
    
    spark.stop()
