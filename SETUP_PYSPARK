echo $JAVA_HOME

readlink -f /usr/bin/java


export JAVA_HOME=/path/to/java
export PATH=$JAVA_HOME/bin:$PATH


echo 'export JAVA_HOME=/path/to/java' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


tar -xzf spark-<version>-bin-hadoop<version>.tgz


export SPARK_HOME=/path/to/spark
export PATH=$SPARK_HOME/bin:$PATH


echo 'export SPARK_HOME=/path/to/spark' >> ~/.bashrc
echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


pip install pyspark

export http_proxy=http://proxy.statsetr.com:80
export https_proxy=http://proxy.statsetr.com:80
wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz


export http_proxy=http://proxy.statstr.com:80
export https_proxy=http://proxy.statstr.com:80
wget --no-check-certificate -e use_proxy=yes -e http_proxy=$http_proxy -e https_proxy=$https_proxy https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz

scp /local/path/to/spark-3.3.0-bin-hadoop3.2.tgz username@vm-ip-address:/opt/spark

cd /opt/spark
sudo tar -xvzf spark-3.3.0-bin-hadoop3.2.tgz

export SPARK_HOME=/opt/spark/spark-3.3.0-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

source ~/.bashrc

spark-shell



sed -i 's/\r$//' /path/to/spark/bin/spark-shell


cat /path/to/spark/bin/spark-shell | tr -d '\r' > /path/to/spark/bin/spark-shell.temp
mv /path/to/spark/bin/spark-shell.temp /path/to/spark/bin/spark-shell


sudo mkdir -p /opt/spark/work
sudo chown azuresuer:azuresuer /opt/spark/work

/opt/spark/bin/spark-submit --master spark://[master-host]:7077 /path/to/your_script.py
