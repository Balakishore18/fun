echo $JAVA_HOME

readlink -f /usr/bin/java


export JAVA_HOME=/path/to/java
export PATH=$JAVA_HOME/bin:$PATH


echo 'export JAVA_HOME=/path/to/java' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


tar -xzf spark-<version>-bin-hadoop<version>.tgz


export SPARK_HOME=/path/to/spark
export PATH=$SPARK_HOME/bin:$PATH


echo 'export SPARK_HOME=/path/to/spark' >> ~/.bashrc
echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


pip install pyspark

export http_proxy=http://proxy.statsetr.com:80
export https_proxy=http://proxy.statsetr.com:80
wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz


export http_proxy=http://proxy.statstr.com:80
export https_proxy=http://proxy.statstr.com:80
wget --no-check-certificate -e use_proxy=yes -e http_proxy=$http_proxy -e https_proxy=$https_proxy https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz

scp /local/path/to/spark-3.3.0-bin-hadoop3.2.tgz username@vm-ip-address:/opt/spark

cd /opt/spark
sudo tar -xvzf spark-3.3.0-bin-hadoop3.2.tgz

export SPARK_HOME=/opt/spark/spark-3.3.0-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

source ~/.bashrc

spark-shell



sed -i 's/\r$//' /path/to/spark/bin/spark-shell


cat /path/to/spark/bin/spark-shell | tr -d '\r' > /path/to/spark/bin/spark-shell.temp
mv /path/to/spark/bin/spark-shell.temp /path/to/spark/bin/spark-shell


sudo mkdir -p /opt/spark/work
sudo chown azuresuer:azuresuer /opt/spark/work

/opt/spark/bin/spark-submit --master spark://[master-host]:7077 /path/to/your_script.py


spark-submit --packages io.delta:delta-core_2.12:<delta_version> your_script.py

wget https://repo1.maven.org/maven2/io/delta/delta-standalone_2.12/0.3.0/delta-standalone_2.12-0.3.0.jar


wget https://repo1.maven.org/maven2/io/delta/delta-standalone_2.12/0.3.0/delta-standalone_2.12-0.3.0.jar

spark-submit --jars /path/to/delta-standalone_2.12-0.3.0.jar your_script.py


from pyspark.sql import SparkSession

# Initialize SparkSession with Delta Lake package and Azure Storage configuration
spark = SparkSession.builder \
    .appName("DeltaTableConnector") \
    .config("spark.jars", "/home/azureuser/.ivy2/jars/delta-standalone-2.12-0.3.0.jar") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config("fs.azure.account.key.<your_account_name>.blob.core.windows.net", "<your_access_key>") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statsetr.com -Dhttp.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statsetr.com -Dhttp.proxyPort=80") \
    .getOrCreate()

# Path to the Delta table
delta_table_path = "dbfs:/user/hive/warehouse/db_uc_mvp.db/test_cust_forecast_delta_silver"

# Read the Delta table
df = spark.read.format("delta").load(delta_table_path)

# Show the data
df.show()

spark-submit --jars /home/azureuser/.ivy2/jars/delta-standalone-2.12-0.3.0.jar /path/to/your_script.py


spark-submit \
  --packages io.delta:delta-core_2.12:0.8.0 \
  --conf "spark.driver.extraJavaOptions=-Dhttp.proxyHost=your_proxy_host -Dhttp.proxyPort=your_proxy_port" \
  --conf "spark.executor.extraJavaOptions=-Dhttp.proxyHost=your_proxy_host -Dhttp.proxyPort=your_proxy_port" \
  /path/to/your_script.py

spark-submit \
  --packages io.delta:delta-core_2.12:2.3.0,org.apache.hadoop:hadoop-azure:3.3.0 \
  --conf "spark.hadoop.fs.azure"="org.apache.hadoop.fs.azure.NativeAzureFileSystem" \
  --conf "spark.driver.extraJavaOptions=-Dhttp.proxyHost=your_proxy_host -Dhttp.proxyPort=your_proxy_port" \
  --conf "spark.executor.extraJavaOptions=-Dhttp.proxyHost=your_proxy_host -Dhttp.proxyPort=your_proxy_port" \
  /path/to/your_script.py



from pyspark.sql import SparkSession
from pyspark.sql.utils import AnalysisException

# Proxy settings
proxy_host = "your.proxy.host"
proxy_port = "your_proxy_port"

# Initialize Spark session with Delta support and proxy settings
spark = SparkSession.builder \
    .appName("DeltaTableConnectivityCheck") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:x.y.z") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.driver.extraJavaOptions", f"-Dhttp.proxyHost={proxy_host} -Dhttp.proxyPort={proxy_port} -Dhttps.proxyHost={proxy_host} -Dhttps.proxyPort={proxy_port}") \
    .config("spark.executor.extraJavaOptions", f"-Dhttp.proxyHost={proxy_host} -Dhttp.proxyPort={proxy_port} -Dhttps.proxyHost={proxy_host} -Dhttps.proxyPort={proxy_port}") \
    .getOrCreate()

# Replace 'x.y.z' with the version of Delta Lake you are using, 
# and make sure it is compatible with your Spark version.

# Path to your Delta table
delta_table_path = "/path/to/your/delta-table"

try:
    # Try to read the Delta table
    df = spark.read.format("delta").load(delta_table_path)
    
    # Show the first few rows of the DataFrame to verify connectivity
    df.show()

    print("Successfully connected to the Delta table.")
except AnalysisException as e:
    print("Failed to connect to the Delta table.")
    print(str(e))

# Stop the Spark session
spark.stop()



spark-submit \
  --packages io.delta:delta-core_2.12:x.y.z \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  --conf "spark.driver.extraJavaOptions=-Dhttp.proxyHost=proxy_host -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_host -Dhttps.proxyPort=proxy_port" \
  --conf "spark.executor.extraJavaOptions=-Dhttp.proxyHost=proxy_host -Dhttp.proxyPort=proxy_port -Dhttps.proxyHost=proxy_host -Dhttps.proxyPort=proxy_port" \
  your_pyspark_script.py


spark-submit --packages com.databricks:spark-avro_2.12:4.0.0,org.apache.hadoop:hadoop-azure:3.2.0 --jars /path/to/your/jar1,/path/to/your/jar2 your-spark-application.py




from pyspark.sql import SparkSession

# Assuming you have the Databricks host and token configured in your environment or Databricks Connect setup
# You would typically set these in your shell environment or in a configuration file
databricks_host = "https://<your-databricks-workspace-url>"
databricks_token = "<your-access-token>"

# Initialize SparkSession with Databricks Connect
spark = SparkSession.builder \
    .appName("DatabricksConnectDeltaExample") \
    .config("spark.databricks.workspace.url", databricks_host) \
    .config("spark.databricks.token", databricks_token) \
    .getOrCreate()

# Example to read a Delta table
delta_table_path = "dbfs:/mnt/delta/<your-delta-table>"
df = spark.read.format("delta").load(delta_table_path)
df.show()

# Perform operations on the DataFrame as needed

# Stop the SparkSession when done
spark.stop()



import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

# Replace these with your actual Databricks workspace URL, access token, Delta table path, and job ID
databricks_host = "https://<your-databricks-workspace-url>"
databricks_token = "<your-access-token>"
delta_table_path = "dbfs:/mnt/delta/<your-delta-table>"
job_id = "<your-job-id>"

def create_spark_session(databricks_host, databricks_token):
    try:
        spark = SparkSession.builder \
            .appName("DatabricksConnectDeltaExample") \
            .config("spark.databricks.workspace.url", databricks_host) \
            .config("spark.databricks.token", databricks_token) \
            .enableHiveSupport() \
            .getOrCreate()
        return spark
    except Exception as e:
        print(f"Error creating Spark session: {e}")
        return None

def connect_to_delta_table(spark, delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        df.show(2, False)
        return df, row_count
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None, 0

def get_latest_version(delta_table):
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()
        if changes_row_count > 0:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show(2, False)
        else:
            print("No changes detected since the last version.")
        return changes_df, changes_row_count
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    url = f"{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Job triggered successfully. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    spark = create_spark_session(databricks_host, databricks_token)
    if spark is None:
        print("Failed to create Spark session. Exiting application.")
        return
    
    df, row_count = connect_to_delta_table(spark, delta_table_path)
    if df is not None and row_count > 0:
        delta_table = DeltaTable.forPath(spark, delta_table_path)
        latest_version = get_latest_version(delta_table)
        
        if latest_version is not None:
            _, changes_row_count = read_change_data_feed(spark, delta_table_path, latest_version - 1)
            if changes_row_count > 0:
                trigger_databricks_job(databricks_host, databricks_token, job_id)
            else:
                print("No changes detected since the last version.")
        else:
            print("Could not determine the latest version for the Delta table.")
    
    spark.stop()

if __name__ == "__main__":
    main()


import requests
from pyspark.sql import SparkSession
from delta.tables import DeltaTable

# Replace with your actual Databricks workspace URL and personal access token
databricks_host = "https://<workspace-name>.<region>.azuredatabricks.net"
databricks_token = "<your-access-token>"

# Replace with your actual Delta table path and job ID
delta_table_path = "dbfs:/mnt/delta/<your-delta-table>"
job_id = "<your-job-id>"

def create_spark_session(databricks_host, databricks_token):
    spark = SparkSession.builder \
        .appName("DatabricksConnectDeltaExample") \
        .config("spark.databricks.workspace.url", databricks_host) \
        .config("spark.databricks.token", databricks_token) \
        .getOrCreate()
    return spark

def connect_to_delta_table(spark, delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        row_count = df.count()
        print(f"Successfully connected to the Delta table. Total rows: {row_count}")
        return df
    except Exception as e:
        print(f"Error connecting to the Delta table: {e}")
        return None

def get_latest_version(delta_table):
    try:
        latest_version = delta_table.history(1).select("version").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    try:
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        current_df = spark.read.format("delta").load(delta_table_path)
        changes_df = current_df.exceptAll(previous_df)
        changes_row_count = changes_df.count()
        if changes_row_count > 0:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            return changes_df, changes_row_count
        else:
            print("No changes detected since the last version.")
            return None, 0
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    url = f"{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Job triggered successfully. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    spark = create_spark_session(databricks_host, databricks_token)
    df = connect_to_delta_table(spark, delta_table_path)
    if df:
        delta_table = DeltaTable.forPath(spark, delta_table_path)
        latest_version = get_latest_version(delta_table)
        if latest_version is not None:
            changes_df, changes_row_count = read_change_data_feed(spark, delta_table_path, latest_version - 1)
            if changes_row_count > 0:
                trigger_databricks_job(databricks_host, databricks_token, job_id)
    spark.stop()

if __name__ == "__main__":
    main()


import requests
from pyspark.sql import SparkSession

# Replace with your actual Databricks workspace URL and personal access token
databricks_host = "https://<workspace-name>.<region>.azuredatabricks.net"
databricks_token = "<your-access-token>"

# Replace with your actual Delta table path and job ID
delta_table_path = "dbfs:/mnt/delta/<your-delta-table>"
job_id = "<your-job-id>"

def create_spark_session(databricks_host, databricks_token):
    spark = SparkSession.builder \
        .appName("DatabricksConnectDeltaExample") \
        .config("spark.databricks.workspace.url", databricks_host) \
        .config("spark.databricks.token", databricks_token) \
        .getOrCreate()
    return spark

def get_latest_version(spark, delta_table_path):
    try:
        df = spark.read.format("delta").load(delta_table_path)
        df.createOrReplaceTempView("deltaTable")
        latest_version = spark.sql("DESCRIBE HISTORY deltaTable LIMIT 1").collect()[0]["version"]
        return latest_version
    except Exception as e:
        print(f"Error getting the latest version: {e}")
        return None

def read_change_data_feed(spark, delta_table_path, previous_version):
    try:
        # Load the Delta table as of the previous version
        previous_df = spark.read.format("delta").option("versionAsOf", previous_version).load(delta_table_path)
        previous_df.createOrReplaceTempView("previousDeltaTable")

        # Load the latest version of the Delta table
        current_df = spark.read.format("delta").load(delta_table_path)
        current_df.createOrReplaceTempView("currentDeltaTable")

        # Find changes by executing a SQL query to subtract the two DataFrames
        changes_df = spark.sql("""
            SELECT * FROM currentDeltaTable
            EXCEPT ALL
            SELECT * FROM previousDeltaTable
        """)
        changes_row_count = changes_df.count()
        if changes_row_count > 0:
            print(f"Detected {changes_row_count} changes since version {previous_version}:")
            changes_df.show()
            return changes_df, changes_row_count
        else:
            print("No changes detected since the last version.")
            return None, 0
    except Exception as e:
        print(f"Error reading change data feed: {e}")
        return None, 0

def trigger_databricks_job(databricks_workspace, databricks_token, job_id):
    url = f"{databricks_workspace}/api/2.0/jobs/run-now"
    headers = {
        "Authorization": f"Bearer {databricks_token}",
        "Content-Type": "application/json"
    }
    payload = {"job_id": job_id}
    response = requests.post(url, headers=headers, json=payload)
    if response.status_code == 200:
        print(f"Job triggered successfully. Response: {response.json()}")
    else:
        print(f"Failed to trigger job. Status: {response.status_code}, Response: {response.text}")

def main():
    spark = create_spark_session(databricks_host, databricks_token)
    latest_version = get_latest_version(spark, delta_table_path)
    if latest_version is not None:
        changes_df, changes_row_count = read_change_data_feed(spark, delta_table_path, latest_version - 1)
        if changes_row_count > 0:
            trigger_databricks_job(databricks_host, databricks_token, job_id)
    spark.stop()

if __name__ == "__main__":
    main()

