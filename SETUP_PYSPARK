echo $JAVA_HOME

readlink -f /usr/bin/java


export JAVA_HOME=/path/to/java
export PATH=$JAVA_HOME/bin:$PATH


echo 'export JAVA_HOME=/path/to/java' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


tar -xzf spark-<version>-bin-hadoop<version>.tgz


export SPARK_HOME=/path/to/spark
export PATH=$SPARK_HOME/bin:$PATH


echo 'export SPARK_HOME=/path/to/spark' >> ~/.bashrc
echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc


pip install pyspark

export http_proxy=http://proxy.statsetr.com:80
export https_proxy=http://proxy.statsetr.com:80
wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz


export http_proxy=http://proxy.statstr.com:80
export https_proxy=http://proxy.statstr.com:80
wget --no-check-certificate -e use_proxy=yes -e http_proxy=$http_proxy -e https_proxy=$https_proxy https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz

scp /local/path/to/spark-3.3.0-bin-hadoop3.2.tgz username@vm-ip-address:/opt/spark

cd /opt/spark
sudo tar -xvzf spark-3.3.0-bin-hadoop3.2.tgz

export SPARK_HOME=/opt/spark/spark-3.3.0-bin-hadoop3.2
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

source ~/.bashrc

spark-shell



sed -i 's/\r$//' /path/to/spark/bin/spark-shell


cat /path/to/spark/bin/spark-shell | tr -d '\r' > /path/to/spark/bin/spark-shell.temp
mv /path/to/spark/bin/spark-shell.temp /path/to/spark/bin/spark-shell


sudo mkdir -p /opt/spark/work
sudo chown azuresuer:azuresuer /opt/spark/work

/opt/spark/bin/spark-submit --master spark://[master-host]:7077 /path/to/your_script.py


spark-submit --packages io.delta:delta-core_2.12:<delta_version> your_script.py

wget https://repo1.maven.org/maven2/io/delta/delta-standalone_2.12/0.3.0/delta-standalone_2.12-0.3.0.jar


wget https://repo1.maven.org/maven2/io/delta/delta-standalone_2.12/0.3.0/delta-standalone_2.12-0.3.0.jar

spark-submit --jars /path/to/delta-standalone_2.12-0.3.0.jar your_script.py


from pyspark.sql import SparkSession

# Initialize SparkSession with Delta Lake package and Azure Storage configuration
spark = SparkSession.builder \
    .appName("DeltaTableConnector") \
    .config("spark.jars", "/home/azureuser/.ivy2/jars/delta-standalone-2.12-0.3.0.jar") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("fs.azure", "org.apache.hadoop.fs.azure.NativeAzureFileSystem") \
    .config("fs.azure.account.key.<your_account_name>.blob.core.windows.net", "<your_access_key>") \
    .config("spark.driver.extraJavaOptions", "-Dhttp.proxyHost=proxy.statsetr.com -Dhttp.proxyPort=80") \
    .config("spark.executor.extraJavaOptions", "-Dhttp.proxyHost=proxy.statsetr.com -Dhttp.proxyPort=80") \
    .getOrCreate()

# Path to the Delta table
delta_table_path = "dbfs:/user/hive/warehouse/db_uc_mvp.db/test_cust_forecast_delta_silver"

# Read the Delta table
df = spark.read.format("delta").load(delta_table_path)

# Show the data
df.show()

