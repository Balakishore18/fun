import pysftp
from azure.cosmos import CosmosClient
import json
import os
import datetime
import time

# SFTP server credentials
sftp_host = 'your_sftp_host'
sftp_port = 22
sftp_username = 'your_sftp_username'
sftp_password = 'your_sftp_password'
sftp_remote_path = 'sftp server path'

# Cosmos DB credentials
cosmosdb_endpoint = 'your_cosmosdb_endpoint'
cosmosdb_key = 'your_cosmosdb_key'
cosmosdb_database_name = 'your_database_name'
cosmosdb_container_name = 'your_container_name'

# File to store the last known state of the SFTP directory
state_file = 'sftp_state.json'

# Connect to Cosmos DB
cosmos_client = CosmosClient(cosmosdb_endpoint, cosmosdb_key)
database = cosmos_client.get_database_client(cosmosdb_database_name)
container = database.get_container_client(cosmosdb_container_name)

def get_last_state():
    if os.path.exists(state_file):
        with open(state_file, 'r') as file:
            return json.load(file)
    return {}

def set_last_state(state):
    with open(state_file, 'w') as file:
        json.dump(state, file)

def log_file_details(file_name, file_stat):
    file_size = file_stat.st_size
    last_modified = datetime.datetime.fromtimestamp(file_stat.st_mtime).isoformat()

    # Log the details to Cosmos DB
    document = {
        'id': file_name + "_" + last_modified,  # Unique ID for the document
        'fileName': file_name,
        'fileSize': file_size,
        'lastModified': last_modified,
        'remoteFilePath': os.path.join(sftp_remote_path, file_name)
    }

    container.create_item(body=document)
    print(f"Details of file '{file_name}' logged to Cosmos DB.")

def check_for_new_files():
    last_state = get_last_state()
    new_state = {}

    with pysftp.Connection(sftp_host, username=sftp_username, password=sftp_password, port=sftp_port) as sftp:
        sftp.chdir(sftp_remote_path)
        for file_name in sftp.listdir():
            file_path = os.path.join(sftp_remote_path, file_name)
            file_stat = sftp.stat(file_path)
            new_state[file_name] = file_stat.st_mtime

            # If the file is new or modified
            if file_name not in last_state or last_state[file_name] != file_stat.st_mtime:
                log_file_details(file_name, file_stat)

    set_last_state(new_state)

# Polling interval in seconds
polling_interval = 60

# Main loop
while True:
    check_for_new_files()
    time.sleep(polling_interval)






# SFTP server connection string
sftp_connection_string = "sftp://your_username:your_password@your_sftp_host:your_sftp_port"




import requests
from requests.auth import HTTPBasicAuth

def get_dag_data(manual_dag_id):
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoint for DAGs and DAG Runs
    airflow_api_endpoint = "http://your-airflow-api-endpoint/api/v1"
    
    try:
        # Make GET request to Airflow API to get DAG information
        dag_info_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_info = dag_info_response.json()

        # Make GET request to Airflow API to get DAG runs
        dag_runs_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_runs = dag_runs_response.json()

        # Print the retrieved data
        print(f"Data for DAG '{manual_dag_id}':")
        print("DAG Information:")
        print(dag_info)
        print("\nDAG Runs:")
        print(dag_runs)

    except Exception as e:
        print(f"An error occurred: {e}")

# Specify the DAG ID manually
dag_id_to_retrieve = "your_manual_dag_id"

# Call the function to get Airflow data for the specified DAG
get_dag_data(dag_id_to_retrieve)


import requests
from requests.auth import HTTPBasicAuth
from azure.cosmos import CosmosClient

def get_and_store_dag_data(cosmos_db_endpoint, cosmos_db_key, cosmos_db_database_id, cosmos_db_container_id, manual_dag_id):
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoint for DAGs and DAG Runs
    airflow_api_endpoint = "http://your-airflow-api-endpoint/api/v1"

    try:
        # Make GET request to Airflow API to get DAG information
        dag_info_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_info = dag_info_response.json()

        # Make GET request to Airflow API to get DAG runs
        dag_runs_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_runs = dag_runs_response.json()

        # Initialize Cosmos DB client
        cosmos_client = CosmosClient(cosmos_db_endpoint, cosmos_db_key)
        database = cosmos_client.get_database_client(cosmos_db_database_id)
        container = database.get_container_client(cosmos_db_container_id)

        # Assuming you want to store the data in Cosmos DB
        # Replace this with your specific data model and container logic
        item = {
            "dag_id": manual_dag_id,
            "dag_info": dag_info,
            "dag_runs": dag_runs
        }

        # Insert the item into Cosmos DB container
        container.upsert_item(item)

        print(f"Data for DAG '{manual_dag_id}' successfully retrieved and stored in Cosmos DB.")

    except Exception as e:
        print(f"An error occurred: {e}")

# Specify the Cosmos DB details
cosmos_db_endpoint = "your_cosmos_db_endpoint"
cosmos_db_key = "your_cosmos_db_key"
cosmos_db_database_id = "your_cosmos_db_database_id"
cosmos_db_container_id = "your_cosmos_db_container_id"

# Specify the DAG ID manually
dag_id_to_retrieve = "your_manual_dag_id"

# Call the function to get Airflow data for the specified DAG and store in Cosmos DB
get_and_store_dag_data(cosmos_db_endpoint, cosmos_db_key, cosmos_db_database_id, cosmos_db_container_id, dag_id_to_retrieve)



import requests
from requests.auth import HTTPBasicAuth
from azure.cosmos import CosmosClient

def get_dag_data_and_store(cosmos_db_endpoint, cosmos_db_key, cosmos_db_database_id, source_container_id, destination_container_id):
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoint for DAGs
    airflow_api_endpoint = "http://your-airflow-api-endpoint/api/v1"

    try:
        # Initialize Cosmos DB client for source container
        source_cosmos_client = CosmosClient(cosmos_db_endpoint, cosmos_db_key)
        source_database = source_cosmos_client.get_database_client(cosmos_db_database_id)
        source_container = source_database.get_container_client(source_container_id)

        # Retrieve DAG ID from the source container
        # Replace this with your actual query logic
        query = "SELECT c.dag_id FROM c"
        items = list(source_container.query_items(query, enable_cross_partition_query=True))

        for item in items:
            manual_dag_id = item["dag_id"]

            # Make GET request to Airflow API to get DAG information
            dag_info_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
            dag_info = dag_info_response.json()

            # Make GET request to Airflow API to get DAG runs
            dag_runs_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
            dag_runs = dag_runs_response.json()

            # Initialize Cosmos DB client for destination container
            destination_cosmos_client = CosmosClient(cosmos_db_endpoint, cosmos_db_key)
            destination_database = destination_cosmos_client.get_database_client(cosmos_db_database_id)
            destination_container = destination_database.get_container_client(destination_container_id)

            # Assuming you want to store the data in the destination Cosmos DB container
            # Replace this with your specific data model and container logic
            stored_item = {
                "dag_id": manual_dag_id,
                "dag_info": dag_info,
                "dag_runs": dag_runs
            }

            # Insert the item into the destination Cosmos DB container
            destination_container.upsert_item(stored_item)

            print(f"Data for DAG '{manual_dag_id}' successfully retrieved from source and stored in destination Cosmos DB.")

    except Exception as e:
        print(f"An error occurred: {e}")

# Specify the Cosmos DB details
cosmos_db_endpoint = "your_cosmos_db_endpoint"
cosmos_db_key = "your_cosmos_db_key"
cosmos_db_database_id = "your_cosmos_db_database_id"

# Specify the source and destination container IDs
source_container_id = "your_source_container_id"
destination_container_id = "your_destination_container_id"

# Call the function to get DAG data from the source container and store it in the destination container
get_dag_data_and_store(cosmos_db_endpoint, cosmos_db_key, cosmos_db_database_id, source_container_id, destination_container_id)


import streamlit as st
import networkx as nx
import matplotlib.pyplot as plt
from airflow.models import DagBag

def get_dag_graph(dag_id):
    dag_bag = DagBag()
    
    if dag_id not in dag_bag.dags:
        return None
    
    dag = dag_bag.get_dag(dag_id)
    
    # Create a directed graph from the DAG
    graph = nx.DiGraph()

    for task_id, task in dag.task_dict.items():
        downstream_task_ids = [downstream_task.task_id for downstream_task in task.downstream_list]
        graph.add_node(task_id, label=task.task_id)
        graph.add_edges_from((task_id, downstream_task_id) for downstream_task_id in downstream_task_ids)

    return graph

def visualize_dag_ui():
    st.title("Airflow DAG Visualization in Streamlit")

    # Specify the DAG ID
    dag_id_to_visualize = st.text_input("Enter DAG ID:", "your_dag_id")

    # Retrieve the DAG graph
    dag_graph = get_dag_graph(dag_id_to_visualize)

    if dag_graph:
        # Plot DAG using Matplotlib
        plt.figure(figsize=(10, 8))
        pos = nx.spring_layout(dag_graph)
        nx.draw(dag_graph, pos, with_labels=True, font_weight='bold', node_size=700, node_color="skyblue", arrowsize=20)
        st.pyplot()

    else:
        st.warning("DAG not found. Please enter a valid DAG ID.")

# Run the Streamlit app
if __name__ == "__main__":
    visualize_dag_ui()
