import pysftp
from azure.cosmos import CosmosClient
import json
import os
import datetime
import time

# SFTP server credentials
sftp_host = 'your_sftp_host'
sftp_port = 22
sftp_username = 'your_sftp_username'
sftp_password = 'your_sftp_password'
sftp_remote_path = 'sftp server path'

# Cosmos DB credentials
cosmosdb_endpoint = 'your_cosmosdb_endpoint'
cosmosdb_key = 'your_cosmosdb_key'
cosmosdb_database_name = 'your_database_name'
cosmosdb_container_name = 'your_container_name'

# File to store the last known state of the SFTP directory
state_file = 'sftp_state.json'

# Connect to Cosmos DB
cosmos_client = CosmosClient(cosmosdb_endpoint, cosmosdb_key)
database = cosmos_client.get_database_client(cosmosdb_database_name)
container = database.get_container_client(cosmosdb_container_name)

def get_last_state():
    if os.path.exists(state_file):
        with open(state_file, 'r') as file:
            return json.load(file)
    return {}

def set_last_state(state):
    with open(state_file, 'w') as file:
        json.dump(state, file)

def log_file_details(file_name, file_stat):
    file_size = file_stat.st_size
    last_modified = datetime.datetime.fromtimestamp(file_stat.st_mtime).isoformat()

    # Log the details to Cosmos DB
    document = {
        'id': file_name + "_" + last_modified,  # Unique ID for the document
        'fileName': file_name,
        'fileSize': file_size,
        'lastModified': last_modified,
        'remoteFilePath': os.path.join(sftp_remote_path, file_name)
    }

    container.create_item(body=document)
    print(f"Details of file '{file_name}' logged to Cosmos DB.")

def check_for_new_files():
    last_state = get_last_state()
    new_state = {}

    with pysftp.Connection(sftp_host, username=sftp_username, password=sftp_password, port=sftp_port) as sftp:
        sftp.chdir(sftp_remote_path)
        for file_name in sftp.listdir():
            file_path = os.path.join(sftp_remote_path, file_name)
            file_stat = sftp.stat(file_path)
            new_state[file_name] = file_stat.st_mtime

            # If the file is new or modified
            if file_name not in last_state or last_state[file_name] != file_stat.st_mtime:
                log_file_details(file_name, file_stat)

    set_last_state(new_state)

# Polling interval in seconds
polling_interval = 60

# Main loop
while True:
    check_for_new_files()
    time.sleep(polling_interval)






# SFTP server connection string
sftp_connection_string = "sftp://your_username:your_password@your_sftp_host:your_sftp_port"




import requests
from requests.auth import HTTPBasicAuth

def get_dag_data(manual_dag_id):
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoint for DAGs and DAG Runs
    airflow_api_endpoint = "http://your-airflow-api-endpoint/api/v1"
    
    try:
        # Make GET request to Airflow API to get DAG information
        dag_info_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_info = dag_info_response.json()

        # Make GET request to Airflow API to get DAG runs
        dag_runs_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_runs = dag_runs_response.json()

        # Print the retrieved data
        print(f"Data for DAG '{manual_dag_id}':")
        print("DAG Information:")
        print(dag_info)
        print("\nDAG Runs:")
        print(dag_runs)

    except Exception as e:
        print(f"An error occurred: {e}")

# Specify the DAG ID manually
dag_id_to_retrieve = "your_manual_dag_id"

# Call the function to get Airflow data for the specified DAG
get_dag_data(dag_id_to_retrieve)


import requests
from requests.auth import HTTPBasicAuth
from azure.cosmos import CosmosClient

def get_and_store_dag_data(cosmos_db_endpoint, cosmos_db_key, cosmos_db_database_id, cosmos_db_container_id, manual_dag_id):
    # Replace with your Airflow credentials
    airflow_username = "your_airflow_username"
    airflow_password = "your_airflow_password"
    
    # Replace with your Airflow API endpoint for DAGs and DAG Runs
    airflow_api_endpoint = "http://your-airflow-api-endpoint/api/v1"

    try:
        # Make GET request to Airflow API to get DAG information
        dag_info_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_info = dag_info_response.json()

        # Make GET request to Airflow API to get DAG runs
        dag_runs_response = requests.get(f"{airflow_api_endpoint}/dags/{manual_dag_id}/dagRuns", auth=HTTPBasicAuth(airflow_username, airflow_password))
        dag_runs = dag_runs_response.json()

        # Initialize Cosmos DB client
        cosmos_client = CosmosClient(cosmos_db_endpoint, cosmos_db_key)
        database = cosmos_client.get_database_client(cosmos_db_database_id)
        container = database.get_container_client(cosmos_db_container_id)

        # Assuming you want to store the data in Cosmos DB
        # Replace this with your specific data model and container logic
        item = {
            "dag_id": manual_dag_id,
            "dag_info": dag_info,
            "dag_runs": dag_runs
        }

        # Insert the item into Cosmos DB container
        container.upsert_item(item)

        print(f"Data for DAG '{manual_dag_id}' successfully retrieved and stored in Cosmos DB.")

    except Exception as e:
        print(f"An error occurred: {e}")

# Specify the Cosmos DB details
cosmos_db_endpoint = "your_cosmos_db_endpoint"
cosmos_db_key = "your_cosmos_db_key"
cosmos_db_database_id = "your_cosmos_db_database_id"
cosmos_db_container_id = "your_cosmos_db_container_id"

# Specify the DAG ID manually
dag_id_to_retrieve = "your_manual_dag_id"

# Call the function to get Airflow data for the specified DAG and store in Cosmos DB
get_and_store_dag_data(cosmos_db_endpoint, cosmos_db_key, cosmos_db_database_id, cosmos_db_container_id, dag_id_to_retrieve)
